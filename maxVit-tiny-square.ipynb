{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Adjust these parameters as needed\n",
    "# ============================================================================\n",
    "# Model: MaxViT-Tiny (maxvit_tiny_tf_512.in1k)\n",
    "# Pretrained weights: pretrained_models/model_tiny.pth\n",
    "# Encoder channels: [64, 64, 128, 256, 512]\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Dataset parameters\n",
    "    IMG_SIZE = (512, 512)  # Image dimensions (height, width)\n",
    "    TRAIN_VAL_SPLIT = 0.9  # Train/validation split ratio\n",
    "    \n",
    "    # DataLoader parameters\n",
    "    BATCH_SIZE = 8  # Batch size for training\n",
    "    NUM_WORKERS = 4  # Set to 0 for Windows, increase for Linux/Mac\n",
    "    PIN_MEMORY = True  # Set to True if using GPU\n",
    "    \n",
    "    # Model architecture parameters\n",
    "    \n",
    "    IN_CHANNELS = 3  # RGB input channels\n",
    "    OUT_CHANNELS = 1  # Binary segmentation output\n",
    "    CBAM_REDUCTION = 16  # Reduction factor for CBAM attention modules\n",
    "    SKIP_PRETRAINED = False  # If True, skip loading pretrained weights (train from scratch or use checkpoint only)\n",
    "    FREEZE_ENCODER = False  # If True, freeze encoder layers (use pretrained embeddings, save compute)\n",
    "    FREEZE_ENCODER_STAGES = [1,2,3]  # List of encoder stages to freeze: [1,2,3,4] or [1,2] etc. None = use FREEZE_ENCODER (freeze all or none)\n",
    "    # Stage mapping: 1=stem+stage1, 2=stage2, 3=stage3, 4=stage4\n",
    "    # Examples: [1,2] freezes first 2 stages, [1,2,3,4] freezes all, None uses FREEZE_ENCODER flag\n",
    "    \n",
    "    # Model Architecture Options\n",
    "    USE_DEEP_SUPERVISION = False  # Enable deep supervision with auxiliary outputs\n",
    "    USE_GRADIENT_CHECKPOINTING = False  # Enable gradient checkpointing to save memory (trades compute for VRAM)\n",
    "    USE_ATROUS_PYRAMID_BOTTLENECK = False  # If True, use atrous (dilated) pyramid bottleneck (ASPP-style) instead of standard bottleneck\n",
    "    USE_IDENTITY_BOTTLENECK = True # If True, use identity bottleneck (no processing, just pass through)\n",
    "    \n",
    "    # Training parameters\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 4e-5\n",
    "    OPTIMIZER = 'adam'  # 'adam', 'adamw', or 'sgd'\n",
    "    WEIGHT_DECAY = 0  # L2 regularization weight\n",
    "    \n",
    "    # Loss function parameters\n",
    "    FOCAL_WEIGHT = 0.95  # Weight for Focal loss (segmentation)\n",
    "    USE_DICE_LOSS = False # Enable/disable Dice loss (if False, only Focal loss is used)\n",
    "    DICE_WEIGHT = 0.05  # Weight for Dice loss (segmentation) - only used if USE_DICE_LOSS=True\n",
    "    DICE_SMOOTH = 1e-6  # Smoothing factor for Dice loss (avoid division by zero)\n",
    "    FOCAL_ALPHA = 0.33  # Alpha parameter for Focal loss (class balancing)\n",
    "    FOCAL_GAMMA = 2.0  # Gamma parameter for Focal loss (focusing parameter)\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    PATIENCE = 99  # Number of epochs to wait before early stopping\n",
    "    MIN_DELTA = 0.0  # Minimum change to qualify as improvement\n",
    "    \n",
    "    # Output directories\n",
    "    OUTPUT_DIR = 'outputs_unet'  # Main output directory\n",
    "    VIZ_DIR = 'outputs_unet/visualizations'  # Directory to save visualizations\n",
    "    CHECKPOINT_DIR = 'outputs_unet/checkpoints'  # Directory to save model checkpoints\n",
    "    \n",
    "    # Visualization parameters\n",
    "    NUM_VIZ_IMAGES = 4  # Number of images to visualize per epoch\n",
    "    \n",
    "    # Model saving parameters\n",
    "    MODEL_SAVE_NAME = 'MaxVit-square-Unet-fraud-best.pth'  # Best model checkpoint name\n",
    "    MODEL_COMPLETE_NAME = 'MaxVit-square-Unet-tiny-fraud-complete.pth'  # Complete model save name\n",
    "    \n",
    "    # Dataset paths\n",
    "    DATASET_ROOT = 'combined_dataset'  # Root folder containing subfolders (casia_copymove, defacto_copymove, science-fraud_copymove)\n",
    "    # Each subfolder should have: train/ and test/ subfolders, each with images/ (PNG files) and masks/ (NPY files)\n",
    "    # Structure: combined_dataset/subfolder/train/images/, combined_dataset/subfolder/test/images/, etc.\n",
    "    \n",
    "    # Dataset selection options\n",
    "    # List of dataset subfolder names to include (None = include all available datasets)\n",
    "    # Examples: ['casia_copymove', 'defacto_copymove'] or ['science-fraud_copymove'] or None (all)\n",
    "    DATASETS_TO_LOAD = ['science-fraud_copymove','defacto_copymove']  # None = load all datasets, or specify list like ['casia_copymove', 'defacto_copymove']\n",
    "    # List of dataset subfolder names to exclude (applied after DATASETS_TO_LOAD if both are set)\n",
    "    # Examples: ['science-fraud_copymove'] to exclude it, or [] to exclude none\n",
    "    DATASETS_TO_SKIP = []  # Empty list = skip none, or specify list like ['science-fraud_copymove']\n",
    "    \n",
    "    # Mask area filtering options\n",
    "    # Filter image-mask pairs based on mask area percentage (0.0 = disabled, 0.01 = 1%, 0.10 = 10%)\n",
    "    FILTER_BY_MASK_AREA = True  # Enable/disable mask area filtering\n",
    "    MIN_MASK_AREA_PERCENT = 0.02  # Minimum mask area as fraction (0.01 = 1% of image)\n",
    "    MAX_MASK_AREA_PERCENT = 0.15  # Maximum mask area as fraction (0.10 = 10% of image)\n",
    "    # Note: Authentic images (empty masks) are always included regardless of area filter\n",
    "    \n",
    "    # Augmentation parameters\n",
    "    USE_AUGMENTATION = True  # Master switch: Enable/disable all data augmentation\n",
    "    \n",
    "    # Spatial augmentation switches (applied to both image and mask)\n",
    "    AUG_ENABLE_ROTATION = True  # Enable/disable rotation augmentation\n",
    "    AUG_ROTATION = 10  # Rotation angle in degrees (±) - only used if AUG_ENABLE_ROTATION=True\n",
    "    AUG_ENABLE_HFLIP = True  # Enable/disable horizontal flip augmentation\n",
    "    AUG_HFLIP = 0.5  # Probability of horizontal flip - only used if AUG_ENABLE_HFLIP=True\n",
    "    AUG_ENABLE_VFLIP = True  # Enable/disable vertical flip augmentation\n",
    "    AUG_VFLIP = 0.5  # Probability of vertical flip - only used if AUG_ENABLE_VFLIP=True\n",
    "    \n",
    "    # Color augmentation switches (applied to image only)\n",
    "    AUG_ENABLE_COLOR = True  # Enable/disable color augmentations (brightness, contrast, saturation)\n",
    "    AUG_BRIGHTNESS = 0.2  # Brightness adjustment range (±) - only used if AUG_ENABLE_COLOR=True\n",
    "    AUG_CONTRAST = 0.2  # Contrast adjustment range (±) - only used if AUG_ENABLE_COLOR=True\n",
    "    AUG_SATURATION = 0.2  # Saturation adjustment range (±) - only used if AUG_ENABLE_COLOR=True\n",
    "    \n",
    "    # Mask smoothing parameters\n",
    "    MASK_GAUSSIAN_BLUR_RADIUS = 1.0 # Gaussian blur radius for mask label smoothing (0.0 = disabled)\n",
    "    \n",
    "    # Per-image normalization parameters\n",
    "    # Normalization flow: [0,1] norm with 1% extreme clipping -> z-norm -> clamp to [-3,3]\n",
    "    USE_PER_IMAGE_MINMAX = True # Enable per-image min-max normalization with 1% extreme clipping (1st-99th percentile) -> [0,1]\n",
    "    USE_PER_IMAGE_ZSCORE = False  # Enable per-image z-score normalization (applied after min-max if enabled)\n",
    "    PER_IMAGE_PERCENTILE_CLIP = True  # Use percentile clipping for min-max (robust to outliers, clips 1% extremes)\n",
    "    PER_IMAGE_LOWER_PERCENTILE = 0.1  # Lower percentile for clipping (1 = 1st percentile, clips bottom 1%)\n",
    "    PER_IMAGE_UPPER_PERCENTILE = 99.9  # Upper percentile for clipping (99 = 99th percentile, clips top 1%)\n",
    "    USE_WEIGHTED_LOSS = True  # Enable weighted loss (higher penalty for missing origins)\n",
    "    \n",
    "    # Size-based loss scaling (linear inverse: 1% area = 10x loss, 10% area = 1x loss)\n",
    "    USE_SIZE_BASED_WEIGHTING = True  # Enable size-based loss scaling\n",
    "    SIZE_WEIGHT_MIN_AREA = 0.01  # Minimum area threshold (1% of image) - gets 10x weight\n",
    "    SIZE_WEIGHT_MAX_AREA = 0.10  # Maximum area threshold (10% of image) - gets 1x weight\n",
    "    SIZE_WEIGHT_MIN = 1.0  # Weight for forgeries >= 10% area\n",
    "    SIZE_WEIGHT_MAX = 10.0  # Weight for forgeries <= 1% area\n",
    "    \n",
    "    # Prediction post-processing parameters\n",
    "    PREDICTION_THRESHOLD = 0.6  # Threshold for binarizing predictions (0.0-1.0)\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "\n",
    "# Print architecture options\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Architecture: UNet with MaxViT blocks (encoder + decoder)\")\n",
    "print(f\"  Encoder: Pretrained MaxViT-Tiny (maxvit_tiny_tf_512.in1k)\")\n",
    "print(f\"  Decoder: MaxViT blocks with skip connections (same blocks as encoder)\")\n",
    "print(f\"  Use Deep Supervision: {config.USE_DEEP_SUPERVISION}\")\n",
    "print(f\"  Gradient Checkpointing: {config.USE_GRADIENT_CHECKPOINTING}\")\n",
    "# Determine bottleneck type\n",
    "if config.USE_IDENTITY_BOTTLENECK:\n",
    "    bottleneck_type = 'Identity (pass-through)'\n",
    "elif config.USE_ATROUS_PYRAMID_BOTTLENECK:\n",
    "    bottleneck_type = 'Atrous Pyramid (ASPP-style)'\n",
    "else:\n",
    "    bottleneck_type = 'Standard UNet'\n",
    "print(f\"  Bottleneck: {bottleneck_type}\")\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Image Size: {config.IMG_SIZE}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Model: MaxViT-Tiny UNet (MaxViT blocks in encoder + decoder)\")\n",
    "\n",
    "print(f\"MaxViT UNet (pretrained encoder, MaxViT decoder blocks)\")\n",
    "print(f\"Skip Pretrained: {'YES' if config.SKIP_PRETRAINED else 'NO'}\")\n",
    "print(f\"Freeze Encoder: {'YES' if config.FREEZE_ENCODER else 'NO'}\")\n",
    "if config.FREEZE_ENCODER_STAGES is not None:\n",
    "    print(f\"Freeze Encoder Stages: {config.FREEZE_ENCODER_STAGES}\")\n",
    "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Optimizer: {config.OPTIMIZER}\")\n",
    "if config.USE_DICE_LOSS:\n",
    "    print(f\"Segmentation Loss: Focal ({config.FOCAL_WEIGHT}, alpha={config.FOCAL_ALPHA}, gamma={config.FOCAL_GAMMA}) + Dice ({config.DICE_WEIGHT})\")\n",
    "else:\n",
    "    print(f\"Segmentation Loss: Focal ({config.FOCAL_WEIGHT}, alpha={config.FOCAL_ALPHA}, gamma={config.FOCAL_GAMMA}) [Dice Loss: OFF]\")\n",
    "print(f\"Patience: {config.PATIENCE}\")\n",
    "print(f\"Train/Val Split: {config.TRAIN_VAL_SPLIT}\")\n",
    "print()\n",
    "print(\"Dataset Selection:\")\n",
    "if config.DATASETS_TO_LOAD is None:\n",
    "    print(f\"  Load All Datasets: YES (all available datasets will be loaded)\")\n",
    "else:\n",
    "    print(f\"  Datasets to Load: {config.DATASETS_TO_LOAD}\")\n",
    "if config.DATASETS_TO_SKIP:\n",
    "    print(f\"  Datasets to Skip: {config.DATASETS_TO_SKIP}\")\n",
    "else:\n",
    "    print(f\"  Datasets to Skip: None (all selected datasets will be loaded)\")\n",
    "print(\"Mask Area Filtering:\")\n",
    "if config.FILTER_BY_MASK_AREA:\n",
    "    print(f\"  Enabled: YES\")\n",
    "    print(f\"  Range: {config.MIN_MASK_AREA_PERCENT*100:.1f}% - {config.MAX_MASK_AREA_PERCENT*100:.1f}%\")\n",
    "    print(f\"  Note: Authentic images (empty masks) are always included\")\n",
    "else:\n",
    "    print(f\"  Enabled: NO (all image-mask pairs will be loaded)\")\n",
    "print()\n",
    "print(\"Augmentation Parameters:\")\n",
    "print(f\"  Master Switch: {'ON' if config.USE_AUGMENTATION else 'OFF'}\")\n",
    "if config.USE_AUGMENTATION:\n",
    "    print(f\"  Spatial: Rotation={'ON' if config.AUG_ENABLE_ROTATION else 'OFF'} (±{config.AUG_ROTATION}°), \"\n",
    "          f\"HFlip={'ON' if config.AUG_ENABLE_HFLIP else 'OFF'} (p={config.AUG_HFLIP}), \"\n",
    "          f\"VFlip={'ON' if config.AUG_ENABLE_VFLIP else 'OFF'} (p={config.AUG_VFLIP})\")\n",
    "    print(f\"  Color: {'ON' if config.AUG_ENABLE_COLOR else 'OFF'} \"\n",
    "          f\"(Brightness=±{config.AUG_BRIGHTNESS}, Contrast=±{config.AUG_CONTRAST}, Saturation=±{config.AUG_SATURATION})\")\n",
    "else:\n",
    "    print(\"  All augmentations disabled\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD AND SAVE PRETRAINED WEIGHTS\n",
    "# ============================================================================\n",
    "# This cell downloads the MaxViT pretrained weights and saves them locally\n",
    "# to avoid redownloading them each time you run the notebook.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "# Create pretrained_models directory if it doesn't exist\n",
    "PRETRAINED_MODELS_DIR = 'pretrained_models'\n",
    "os.makedirs(PRETRAINED_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Check if weights already exist\n",
    "# Prioritize model_tiny.pth first for MaxViT-Tiny\n",
    "weight_files = ['model_tiny.pth']\n",
    "local_weights_path = None\n",
    "weights_exist = False\n",
    "\n",
    "for weight_file in weight_files:\n",
    "    weight_path = os.path.join(PRETRAINED_MODELS_DIR, weight_file)\n",
    "    if os.path.exists(weight_path):\n",
    "        local_weights_path = weight_path\n",
    "        weights_exist = True\n",
    "        print(f\"✓ Pretrained weights already exist at: {local_weights_path}\")\n",
    "        print(\"  Skipping download. Delete this file if you want to redownload.\")\n",
    "        break\n",
    "\n",
    "# Download and save weights if they don't exist\n",
    "if not weights_exist:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DOWNLOADING PRETRAINED WEIGHTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set environment variables for faster download\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "    os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'  # 10 minutes timeout\n",
    "    \n",
    "    try:\n",
    "        print(\"Downloading MaxViT-Tiny pretrained weights from Hugging Face...\")\n",
    "        # Download the model with pretrained weights (num_classes=0 removes classifier head)\n",
    "        maxvit = timm.create_model(\n",
    "            'maxvit_tiny_tf_512.in1k',\n",
    "            pretrained=True,\n",
    "            num_classes=0  # Remove classifier head\n",
    "        )\n",
    "        \n",
    "        # Save the state dict to pretrained_models folder\n",
    "        save_path = os.path.join(PRETRAINED_MODELS_DIR, 'model_tiny.pth')\n",
    "        torch.save(maxvit.state_dict(), save_path)\n",
    "        \n",
    "        print(f\"✓ Successfully downloaded and saved pretrained weights!\")\n",
    "        print(f\"  Saved to: {save_path}\")\n",
    "        print(f\"  File size: {os.path.getsize(save_path) / (1024**2):.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Failed to download with primary model name: {e}\")\n",
    "        print(\"Trying alternative model name...\")\n",
    "        try:\n",
    "            maxvit = timm.create_model(\n",
    "                'maxvit_tiny_tf_512.in1k',\n",
    "                pretrained=True,\n",
    "                num_classes=0\n",
    "            )\n",
    "            \n",
    "            # Save the state dict\n",
    "            save_path = os.path.join(PRETRAINED_MODELS_DIR, 'model_tiny.pth')\n",
    "            torch.save(maxvit.state_dict(), save_path)\n",
    "            \n",
    "            print(f\"✓ Successfully downloaded and saved pretrained weights!\")\n",
    "            print(f\"  Saved to: {save_path}\")\n",
    "            print(f\"  File size: {os.path.getsize(save_path) / (1024**2):.2f} MB\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"✗ Error: Failed to download pretrained weights: {e2}\")\n",
    "            print(\"  The model will be created without pretrained weights.\")\n",
    "            print(\"  You can train from scratch or manually download the weights.\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9439,
     "status": "ok",
     "timestamp": 1751816052150,
     "user": {
      "displayName": "JAYESH SHARMA",
      "userId": "10181368842352240411"
     },
     "user_tz": -330
    },
    "id": "b645LenOP6Ni",
    "outputId": "f410e9be-e14b-4a3b-8291-de9cfaedf6df"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module (SE-Net style)\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # Apply channel reduction (bottleneck)\n",
    "        reduced_channels = max(1, in_channels // reduction)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, reduced_channels, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W)\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module (Channel + Spatial)\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply channel attention first\n",
    "        x = x * self.channel_attention(x)\n",
    "        # Then apply spatial attention\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "class AtrousPyramidBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling (ASPP) style bottleneck for multi-scale feature extraction.\n",
    "    Uses parallel convolutions with different dilation rates to capture features at multiple scales.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dilation_rates=[1, 2, 4, 8]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Input channel count\n",
    "            out_channels: Output channel count (same as input for bottleneck)\n",
    "            dilation_rates: List of dilation rates for parallel convolutions\n",
    "        \"\"\"\n",
    "        super(AtrousPyramidBottleneck, self).__init__()\n",
    "        \n",
    "        # Calculate padding for each dilation rate to maintain spatial size\n",
    "        # For 3x3 conv with dilation d: padding = d\n",
    "        self.dilation_rates = dilation_rates\n",
    "        \n",
    "        # Parallel convolutions with different dilation rates\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        for dilation in dilation_rates:\n",
    "            self.conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                             padding=dilation, dilation=dilation, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Global average pooling branch (1x1 output)\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Fusion convolution: concatenate all branches and reduce channels\n",
    "        # Total channels: out_channels * (len(dilation_rates) + 1) [parallel convs + global pool]\n",
    "        total_channels = out_channels * (len(dilation_rates) + 1)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(total_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (B, C, H, W) - same spatial size\n",
    "        \"\"\"\n",
    "        # Get spatial dimensions\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Apply parallel convolutions with different dilation rates\n",
    "        branch_outputs = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            branch_outputs.append(conv_block(x))\n",
    "        \n",
    "        # Global average pooling branch (upsample to original size)\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        branch_outputs.append(global_feat)\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        out = torch.cat(branch_outputs, dim=1)\n",
    "        \n",
    "        # Fusion to reduce channels\n",
    "        out = self.fusion(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MaxViTDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    MaxViT-based decoder block with skip connection from corresponding encoder level.\n",
    "    Uses MaxViT blocks (same as encoder) for feature processing.\n",
    "    Creates decoder blocks that mirror the encoder structure using the same MaxViT block types.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, skip_channels, out_channels, num_blocks=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Channel count from previous decoder level (after upsampling)\n",
    "            skip_channels: Channel count from corresponding encoder skip connection\n",
    "            out_channels: Output channel count for this decoder level\n",
    "            num_blocks: Number of MaxViT blocks to use (default: 2, matching encoder structure)\n",
    "        \"\"\"\n",
    "        super(MaxViTDecoderBlock, self).__init__()\n",
    "        \n",
    "        # Project concatenated features to out_channels\n",
    "        # After concatenation: in_channels + skip_channels -> out_channels\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        \n",
    "        # Create MaxViT blocks for feature processing (same as encoder)\n",
    "        # We'll create blocks that match the encoder structure but without downsampling\n",
    "        from timm.models.maxxvit import MaxxVitBlock, MaxxVitConvCfg, MaxxVitTransformerCfg\n",
    "        \n",
    "        self.maxvit_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Configure MaxViT blocks with proper window and grid sizes\n",
    "        # These match the MaxViT-Tiny architecture used in the encoder\n",
    "        # Window size of 16x16 divides evenly into all decoder spatial sizes (32, 64, 128, 256, 512)\n",
    "        conv_cfg = MaxxVitConvCfg()  # Use default conv config\n",
    "        transformer_cfg = MaxxVitTransformerCfg(\n",
    "            window_size=(16, 16),  # Same as encoder - divides evenly into all spatial sizes\n",
    "            grid_size=(16, 16),    # Same as encoder\n",
    "        )\n",
    "        \n",
    "        # Create blocks similar to encoder but without downsampling\n",
    "        # Each block processes features at the same resolution\n",
    "        for i in range(num_blocks):\n",
    "            # Create a MaxViT block with stride=1 (no downsampling)\n",
    "            # This matches the encoder block structure but processes at same resolution\n",
    "            block = MaxxVitBlock(\n",
    "                dim=out_channels,\n",
    "                dim_out=out_channels,  # Same output dimension (no channel change)\n",
    "                stride=1,  # No downsampling - key difference from encoder\n",
    "                conv_cfg=conv_cfg,\n",
    "                transformer_cfg=transformer_cfg,\n",
    "            )\n",
    "            self.maxvit_blocks.append(block)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Feature from previous decoder level (already upsampled)\n",
    "            skip: Skip connection from corresponding encoder level\n",
    "        Returns:\n",
    "            Output feature map\n",
    "        \"\"\"\n",
    "        # Concatenate upsampled feature with skip connection\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        \n",
    "        # Project to target channels\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Apply MaxViT blocks (same as encoder structure, but no downsampling)\n",
    "        for block in self.maxvit_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MaxViT_CBAM_UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, r=16, skip_pretrained=False, use_deep_supervision=True, use_gradient_checkpointing=False, use_atrous_pyramid_bottleneck=False, use_identity_bottleneck=False):\n",
    "        super(MaxViT_CBAM_UNet, self).__init__()\n",
    "        \n",
    "        self.use_deep_supervision = use_deep_supervision\n",
    "        self.use_gradient_checkpointing = use_gradient_checkpointing\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        PRETRAINED_MODELS_DIR = 'pretrained_models'\n",
    "        os.makedirs(PRETRAINED_MODELS_DIR, exist_ok=True)\n",
    "        \n",
    "        # Check for pre-downloaded model_tiny.pth\n",
    "        local_weights_path = os.path.join(PRETRAINED_MODELS_DIR, 'model_tiny.pth')\n",
    "        \n",
    "        # Create model without pretrained weights (will be random initialization if no weights found)\n",
    "        maxvit = timm.create_model('maxvit_tiny_tf_512.in1k', pretrained=False, num_classes=0)\n",
    "        \n",
    "        # Try to load pre-downloaded weights if they exist and skip_pretrained is False\n",
    "        if not skip_pretrained and os.path.exists(local_weights_path):\n",
    "            try:\n",
    "                print(f\"Loading MaxViT from pre-downloaded weights: {local_weights_path}\")\n",
    "                state_dict = torch.load(local_weights_path, map_location='cpu')\n",
    "                \n",
    "                # Filter state dict to match model architecture (remove classifier head keys)\n",
    "                model_dict = maxvit.state_dict()\n",
    "                filtered_dict = {}\n",
    "                for k, v in state_dict.items():\n",
    "                    # Skip classifier head keys (head.* or classifier.*)\n",
    "                    if 'head.' in k or 'classifier.' in k:\n",
    "                        continue\n",
    "                    if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                        filtered_dict[k] = v\n",
    "                \n",
    "                missing_keys, unexpected_keys = maxvit.load_state_dict(filtered_dict, strict=False)\n",
    "                if missing_keys:\n",
    "                    print(f\"  Note: {len(missing_keys)} keys not loaded\")\n",
    "                if unexpected_keys:\n",
    "                    print(f\"  Note: {len(unexpected_keys)} unexpected keys ignored (including classifier head)\")\n",
    "                \n",
    "                print(\"✓ Successfully loaded pretrained weights from local file!\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Failed to load from local file: {e}\")\n",
    "                print(\"  Initializing model with random weights...\")\n",
    "        elif skip_pretrained:\n",
    "            print(\"Creating MaxViT model without pretrained weights (skip_pretrained=True)...\")\n",
    "            print(\"  Initializing model with random weights...\")\n",
    "        else:\n",
    "            print(\"No pre-downloaded model.pth found in pretrained_models/\")\n",
    "            print(\"  Initializing model with random weights...\")\n",
    "            print(\"  Tip: Run the 'Download and Save Pretrained Weights' cell first to download weights.\")\n",
    "        \n",
    "        # MaxViT Base feature dimensions (for 512x512 input):\n",
    "        # Stem: 64 channels, H/2 x W/2 (256x256)\n",
    "        # Stage 1: 96 channels, H/4 x W/4 (128x128)\n",
    "        # Stage 2: 192 channels, H/8 x W/8 (64x64)\n",
    "        # Stage 3: 384 channels, H/16 x W/16 (32x32)\n",
    "        # Stage 4: 768 channels, H/32 x W/32 (16x16)\n",
    "        \n",
    "        # Extract encoder stages from MaxViT\n",
    "        # Access the internal structure of MaxViT\n",
    "        self.encoder1 = nn.Identity()  # Input: (B, 3, H, W)\n",
    "        \n",
    "        # Get the stem and stages from MaxViT\n",
    "        # MaxViT structure: stem -> stages[0] -> stages[1] -> stages[2] -> stages[3]\n",
    "        self.stem = maxvit.stem  # Stem: (B, 3, H, W) -> (B, 64, H/2, W/2)\n",
    "        self.stage1 = maxvit.stages[0]  # Stage 1: (B, 64, H/2, W/2) -> (B, 64, H/4, W/4) [MaxViT-Tiny]\n",
    "        self.stage2 = maxvit.stages[1]  # Stage 2: (B, 64, H/4, W/4) -> (B, 128, H/8, W/8) [MaxViT-Tiny]\n",
    "        self.stage3 = maxvit.stages[2]  # Stage 3: (B, 128, H/8, W/8) -> (B, 256, H/16, W/16) [MaxViT-Tiny]\n",
    "        self.stage4 = maxvit.stages[3]  # Stage 4: (B, 256, H/16, W/16) -> (B, 512, H/32, W/32) [MaxViT-Tiny]\n",
    "        \n",
    "        # Encoder blocks\n",
    "        # We'll use stem output separately for skip connection\n",
    "        self.encoder2 = nn.Sequential(self.stem, self.stage1)  # Out: (B, 64, H/4, W/4) [MaxViT-Tiny]\n",
    "        self.encoder3 = self.stage2  # Out: (B, 128, H/8, W/8) [MaxViT-Tiny]\n",
    "        self.encoder4 = self.stage3  # Out: (B, 256, H/16, W/16) [MaxViT-Tiny]\n",
    "        self.encoder5 = self.stage4  # Out: (B, 512, H/32, W/32) [MaxViT-Tiny]\n",
    "        \n",
    "        # Bottleneck: identity, atrous pyramid (ASPP-style), or standard UNet\n",
    "        bottleneck_channels = 512  # Same as encoder5 output\n",
    "        if use_identity_bottleneck:\n",
    "            # Identity bottleneck: no processing, just pass through\n",
    "            self.bottleneck = nn.Identity()\n",
    "        elif use_atrous_pyramid_bottleneck:\n",
    "            # Atrous pyramid bottleneck: multi-scale feature extraction with parallel dilated convolutions\n",
    "            self.bottleneck = AtrousPyramidBottleneck(\n",
    "                in_channels=bottleneck_channels,\n",
    "                out_channels=bottleneck_channels,\n",
    "                dilation_rates=[1, 2, 4, 8]  # Different dilation rates for multi-scale features\n",
    "            )\n",
    "        else:\n",
    "            # Standard UNet bottleneck: 2 conv blocks (Conv2d -> BN -> ReLU)\n",
    "            self.bottleneck = nn.Sequential(\n",
    "                nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(bottleneck_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(bottleneck_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        # Output: (B, 512, H/32, W/32) [MaxViT-Tiny]\n",
    "\n",
    "        # 3-Stage Decoder (matching paper: D1, D2, D3 correspond to S1, S2, S3)\n",
    "        # Paper: \"The decoder is made up of three stages, D1 to D3, matching with S1 to S3 stages of the encoder\"\n",
    "        # Decoder channels at each level\n",
    "        d3_ch, d2_ch, d1_ch = 256, 128, 64\n",
    "        \n",
    "        # Decoder 3 (D3): Upsample bottleneck (512) and concat with encoder4 (256) [S3]\n",
    "        # Bottleneck is at H/32, encoder4 (S3) is at H/16\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, d3_ch, kernel_size=2, stride=2)\n",
    "        self.decoder3 = MaxViTDecoderBlock(\n",
    "            in_channels=d3_ch,  # After upconv from bottleneck\n",
    "            skip_channels=256,  # encoder4 (S3) output\n",
    "            out_channels=d3_ch,\n",
    "            num_blocks=2  # Couple of hybrid MaxViT blocks as per paper\n",
    "        )\n",
    "        \n",
    "        # Decoder 2 (D2): Upsample d3 (256) and concat with encoder3 (128) [S2]\n",
    "        # D3 is at H/16, encoder3 (S2) is at H/8\n",
    "        self.upconv2 = nn.ConvTranspose2d(d3_ch, d2_ch, kernel_size=2, stride=2)\n",
    "        self.decoder2 = MaxViTDecoderBlock(\n",
    "            in_channels=d2_ch,  # After upconv from decoder3\n",
    "            skip_channels=128,  # encoder3 (S2) output\n",
    "            out_channels=d2_ch,\n",
    "            num_blocks=2  # Couple of hybrid MaxViT blocks as per paper\n",
    "        )\n",
    "        \n",
    "        # Decoder 1 (D1): Upsample d2 (128) and concat with encoder2 (64) [S1]\n",
    "        # D2 is at H/8, encoder2 (S1) is at H/4\n",
    "        self.upconv1 = nn.ConvTranspose2d(d2_ch, d1_ch, kernel_size=2, stride=2)\n",
    "        self.decoder1 = MaxViTDecoderBlock(\n",
    "            in_channels=d1_ch,  # After upconv from decoder2\n",
    "            skip_channels=64,   # encoder2 (S1) output\n",
    "            out_channels=d1_ch,\n",
    "            num_blocks=2  # Couple of hybrid MaxViT blocks as per paper\n",
    "        )\n",
    "        \n",
    "        # Final upsampling: D1 is at H/4, need to upsample 4x to H\n",
    "        # Paper: \"feature maps of shape 64 × H/4 × W/4 are up-sampled four times\"\n",
    "        self.final_upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Main output head: reduce channels from 64 to C (number of classes)\n",
    "        self.conv = nn.Conv2d(d1_ch, out_channels, kernel_size=1)\n",
    "        \n",
    "        self.training_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.training_iou = []\n",
    "        self.eval_iou = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Import checkpoint here to avoid issues\n",
    "        from torch.utils.checkpoint import checkpoint\n",
    "        \n",
    "        # Encoder path\n",
    "        e1 = self.encoder1(x)  # (B, 3, H, W) - original input\n",
    "        \n",
    "        # Get stem output for skip connection\n",
    "        stem_out = self.stem(e1)  # (B, 64, H/2, W/2)\n",
    "        \n",
    "        # Encoder stages\n",
    "        e2 = self.encoder2(e1)  # (B, 64, H/4, W/4) - stem + stage1 [MaxViT-Tiny]\n",
    "        e3 = self.encoder3(e2)  # (B, 128, H/8, W/8) - stage2 [MaxViT-Tiny]\n",
    "        e4 = self.encoder4(e3)  # (B, 256, H/16, W/16) - stage3 [MaxViT-Tiny]\n",
    "        e5 = self.encoder5(e4)  # (B, 512, H/32, W/32) - stage4 [MaxViT-Tiny]\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(e5)  # (B, 512, H/32, W/32) [MaxViT-Tiny]\n",
    "        \n",
    "        # 3-Stage Decoder (D1, D2, D3 matching encoder stages S1, S2, S3)\n",
    "        \n",
    "        # Decoder 3 (D3): Upsample bottleneck and concat with encoder4 (e4) [S3]\n",
    "        d3_up = self.upconv3(bottleneck)  # (B, 256, H/16, W/16)\n",
    "        if self.use_gradient_checkpointing and self.training:\n",
    "            d3 = checkpoint(self.decoder3, d3_up, e4, use_reentrant=False)\n",
    "        else:\n",
    "            d3 = self.decoder3(d3_up, e4)\n",
    "        \n",
    "        # Decoder 2 (D2): Upsample d3 and concat with encoder3 (e3) [S2]\n",
    "        d2_up = self.upconv2(d3)  # (B, 128, H/8, W/8)\n",
    "        if self.use_gradient_checkpointing and self.training:\n",
    "            d2 = checkpoint(self.decoder2, d2_up, e3, use_reentrant=False)\n",
    "        else:\n",
    "            d2 = self.decoder2(d2_up, e3)\n",
    "        \n",
    "        # Decoder 1 (D1): Upsample d2 and concat with encoder2 (e2) [S1]\n",
    "        d1_up = self.upconv1(d2)  # (B, 64, H/4, W/4)\n",
    "        if self.use_gradient_checkpointing and self.training:\n",
    "            d1 = checkpoint(self.decoder1, d1_up, e2, use_reentrant=False)\n",
    "        else:\n",
    "            d1 = self.decoder1(d1_up, e2)\n",
    "        \n",
    "        # Final upsampling: 4x from H/4 to H (as per paper)\n",
    "        d1_upsampled = self.final_upsample(d1)  # (B, 64, H, W)\n",
    "        \n",
    "        # Output layer: reduce channels from 64 to C (number of classes)\n",
    "        output = self.conv(d1_upsampled)  # (B, 1, H, W) - logits\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Check for existing checkpoint before creating model\n",
    "checkpoint_dir = config.CHECKPOINT_DIR\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Checkpoint to use (from config)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, config.MODEL_SAVE_NAME)\n",
    "\n",
    "checkpoint_found = False\n",
    "checkpoint_to_load = None\n",
    "\n",
    "# Check for checkpoint saved by this notebook\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint_to_load = checkpoint_path\n",
    "    checkpoint_found = True\n",
    "    print(f\"✓ Found checkpoint: {checkpoint_path}\")\n",
    "    print(f\"  Loading from checkpoint instead of pretrained weights...\")\n",
    "    print(f\"  Checkpoint file: {checkpoint_to_load}\")\n",
    "\n",
    "# Determine if we should skip pretrained weights\n",
    "# Skip if: config option is True OR checkpoint is found (checkpoint will overwrite pretrained anyway)\n",
    "skip_pretrained = config.SKIP_PRETRAINED or checkpoint_found\n",
    "\n",
    "if config.SKIP_PRETRAINED and not checkpoint_found:\n",
    "    print(f\"  Skipping pretrained weights (SKIP_PRETRAINED=True). Training from scratch or using checkpoint only.\")\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pass skip_pretrained flag to model initialization\n",
    "model = MaxViT_CBAM_UNet(\n",
    "    in_channels=config.IN_CHANNELS, \n",
    "    out_channels=config.OUT_CHANNELS,\n",
    "    r=config.CBAM_REDUCTION,\n",
    "    skip_pretrained=skip_pretrained,\n",
    "    use_deep_supervision=config.USE_DEEP_SUPERVISION,\n",
    "    use_gradient_checkpointing=config.USE_GRADIENT_CHECKPOINTING,\n",
    "    use_atrous_pyramid_bottleneck=config.USE_ATROUS_PYRAMID_BOTTLENECK,\n",
    "    use_identity_bottleneck=config.USE_IDENTITY_BOTTLENECK\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint if found (this will overwrite pretrained weights)\n",
    "if checkpoint_found:\n",
    "    try:\n",
    "        if checkpoint_to_load.endswith('.pt'):\n",
    "            # Complete checkpoint format\n",
    "            checkpoint = torch.load(checkpoint_to_load, map_location=device)\n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(f\"✓ Successfully loaded model from complete checkpoint!\")\n",
    "                if 'val_loss' in checkpoint:\n",
    "                    print(f\"  Checkpoint validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "                if 'epoch' in checkpoint:\n",
    "                    print(f\"  Checkpoint epoch: {checkpoint['epoch']}\")\n",
    "            else:\n",
    "                # Treat as state dict\n",
    "                model.load_state_dict(checkpoint)\n",
    "                print(f\"✓ Successfully loaded model from checkpoint!\")\n",
    "        else:\n",
    "            # State dict format\n",
    "            model.load_state_dict(torch.load(checkpoint_to_load, map_location=device))\n",
    "            print(f\"✓ Successfully loaded model from checkpoint!\")\n",
    "        print(f\"  Model weights loaded from: {checkpoint_to_load}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Failed to load checkpoint: {e}\")\n",
    "        if config.SKIP_PRETRAINED:\n",
    "            print(f\"  Continuing without pretrained weights (SKIP_PRETRAINED=True).\")\n",
    "        else:\n",
    "            print(f\"  Continuing with pretrained weights instead...\")\n",
    "else:\n",
    "    if config.SKIP_PRETRAINED:\n",
    "        print(f\"No checkpoint found. Training from scratch (SKIP_PRETRAINED=True).\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found. Using pretrained weights from MaxViT encoder.\")\n",
    "\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Freeze encoder layers if enabled\n",
    "# Support selective stage freezing via FREEZE_ENCODER_STAGES\n",
    "if config.FREEZE_ENCODER_STAGES is not None:\n",
    "    # Selective stage freezing: FREEZE_ENCODER_STAGES = [1,2,3,4] or [1,2] etc.\n",
    "    # Stage mapping: 1=stem+stage1, 2=stage2, 3=stage3, 4=stage4\n",
    "    freeze_stages = set(config.FREEZE_ENCODER_STAGES)\n",
    "    \n",
    "    frozen_components = []\n",
    "    \n",
    "    if 1 in freeze_stages:\n",
    "        # Freeze Stage 1: stem + stage1 (encoder2)\n",
    "        for param in model.stem.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.stage1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.encoder2.parameters():\n",
    "            param.requires_grad = False\n",
    "        frozen_components.append(\"Stage 1 (stem+stage1)\")\n",
    "    \n",
    "    if 2 in freeze_stages:\n",
    "        # Freeze Stage 2: stage2 (encoder3)\n",
    "        for param in model.stage2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.encoder3.parameters():\n",
    "            param.requires_grad = False\n",
    "        frozen_components.append(\"Stage 2 (stage2)\")\n",
    "    \n",
    "    if 3 in freeze_stages:\n",
    "        # Freeze Stage 3: stage3 (encoder4)\n",
    "        for param in model.stage3.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.encoder4.parameters():\n",
    "            param.requires_grad = False\n",
    "        frozen_components.append(\"Stage 3 (stage3)\")\n",
    "    \n",
    "    if 4 in freeze_stages:\n",
    "        # Freeze Stage 4: stage4 (encoder5)\n",
    "        for param in model.stage4.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.encoder5.parameters():\n",
    "            param.requires_grad = False\n",
    "        frozen_components.append(\"Stage 4 (stage4)\")\n",
    "    \n",
    "    # Note: encoder1 (Identity) and Bottleneck are NOT frozen - they should always be trainable\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"\\n✓ Encoder stages frozen: {config.FREEZE_ENCODER_STAGES}\")\n",
    "    print(f\"  Frozen components: {', '.join(frozen_components)}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/(trainable_params+frozen_params):.1f}%)\")\n",
    "    print(f\"  Frozen parameters: {frozen_params:,} ({100*frozen_params/(trainable_params+frozen_params):.1f}%)\")\n",
    "    \n",
    "elif config.FREEZE_ENCODER:\n",
    "    # Legacy behavior: freeze all encoder components (MaxViT backbone)\n",
    "    for param in model.encoder1.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.stem.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.stage1.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.stage2.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.stage3.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.stage4.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.encoder2.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.encoder3.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.encoder4.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.encoder5.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Note: Bottleneck is NOT frozen - it should always be trainable\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"\\n✓ Encoder frozen (using pretrained embeddings)\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/(trainable_params+frozen_params):.1f}%)\")\n",
    "    print(f\"  Frozen parameters: {frozen_params:,} ({100*frozen_params/(trainable_params+frozen_params):.1f}%)\")\n",
    "else:\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nEncoder: Trainable (all {trainable_params:,} parameters will be updated)\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for binary segmentation with optional spatial weights\"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets, weights=None):\n",
    "        # Calculate per-pixel Dice loss\n",
    "        # Flatten tensors (keep spatial dimensions for weighting)\n",
    "        batch_size = predictions.shape[0]\n",
    "        predictions_flat = predictions.view(batch_size, -1)\n",
    "        targets_flat = targets.view(batch_size, -1)\n",
    "        \n",
    "        # Calculate Dice coefficient per sample\n",
    "        intersection = (predictions_flat * targets_flat).sum(dim=1)\n",
    "        union = predictions_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Clamp dice to [0, 1] to prevent numerical issues\n",
    "        dice = torch.clamp(dice, 0.0, 1.0)\n",
    "        dice_loss = 1 - dice\n",
    "        \n",
    "        # If weights provided, calculate weighted average\n",
    "        if weights is not None:\n",
    "            # Average weight per sample (for weighting the loss)\n",
    "            weights_flat = weights.view(batch_size, -1)\n",
    "            sample_weights = weights_flat.mean(dim=1)  # Average weight per sample\n",
    "            # Weighted mean\n",
    "            dice_loss = (dice_loss * sample_weights).sum() / (sample_weights.sum() + 1e-8)\n",
    "        else:\n",
    "            dice_loss = dice_loss.mean()\n",
    "        \n",
    "        return dice_loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for binary segmentation with optional spatial weights\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets, weights=None):\n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Calculate BCE loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        \n",
    "        # Calculate p_t (probability of true class)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        \n",
    "        # Calculate alpha_t (alpha for true class)\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        focal_weight = alpha_t * (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Apply focal weight to BCE loss\n",
    "        focal_loss = focal_weight * bce_loss\n",
    "        \n",
    "        # Apply spatial weights if provided\n",
    "        if weights is not None:\n",
    "            focal_loss = focal_loss * weights\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class CombinedLossWithDeepSupervision(nn.Module):\n",
    "    \"\"\"Combined Focal and Dice Loss with Deep Supervision support\"\"\"\n",
    "    def __init__(self, focal_weight=0.5, dice_weight=0.5, dice_smooth=1e-6, \n",
    "                 focal_alpha=0.25, focal_gamma=2.0, deep_supervision_weights=None, use_dice_loss=True):\n",
    "        super(CombinedLossWithDeepSupervision, self).__init__()\n",
    "        self.focal_weight = focal_weight\n",
    "        self.use_dice_loss = use_dice_loss\n",
    "        self.dice_weight = dice_weight if use_dice_loss else 0.0\n",
    "        self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "        if use_dice_loss:\n",
    "            self.dice_loss = DiceLoss(smooth=dice_smooth)\n",
    "        else:\n",
    "            self.dice_loss = None\n",
    "        \n",
    "        # Weights for deep supervision outputs (main, aux5, aux4, aux3, aux2)\n",
    "        if deep_supervision_weights is None:\n",
    "            self.deep_supervision_weights = [1.0, 0.8, 0.6, 0.4, 0.2]\n",
    "        else:\n",
    "            self.deep_supervision_weights = deep_supervision_weights\n",
    "    \n",
    "    def forward(self, outputs, mask_targets, weight_map=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs: Single tensor (main output only)\n",
    "            mask_targets: Ground truth masks\n",
    "            weight_map: Optional spatial weight map\n",
    "        \"\"\"\n",
    "        # Handle both tuple (for compatibility) and single output\n",
    "        if isinstance(outputs, tuple):\n",
    "            # If tuple provided, use only main output (first element)\n",
    "            main_output = outputs[0]\n",
    "        else:\n",
    "            main_output = outputs\n",
    "        \n",
    "        # Single output loss calculation\n",
    "        seg_focal = self.focal_loss(main_output, mask_targets, weight_map)\n",
    "        if self.use_dice_loss:\n",
    "            mask_probs = torch.sigmoid(main_output)\n",
    "            seg_dice = self.dice_loss(mask_probs, mask_targets, weight_map)\n",
    "            return self.focal_weight * seg_focal + self.dice_weight * seg_dice\n",
    "        else:\n",
    "            return self.focal_weight * seg_focal\n",
    "\n",
    "def calculate_iou(logits_or_probs, targets, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) for binary segmentation.\n",
    "    \n",
    "    Args:\n",
    "        logits_or_probs: Model output logits (B, 1, H, W) or probabilities (B, 1, H, W)\n",
    "                        Can be a tuple for deep supervision (uses first element)\n",
    "        targets: Ground truth masks tensor (B, 1, H, W) - values in [0, 1]\n",
    "        threshold: Threshold for binarizing predictions (default: 0.5)\n",
    "        smooth: Smoothing factor to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        Mean IoU score (scalar tensor)\n",
    "    \"\"\"\n",
    "    # Handle deep supervision outputs (tuple) or single output (tensor)\n",
    "    if isinstance(logits_or_probs, tuple):\n",
    "        main_output = logits_or_probs[0]  # Use main output for metrics\n",
    "    else:\n",
    "        main_output = logits_or_probs\n",
    "    \n",
    "    # Apply sigmoid if logits (values outside [0,1] range), otherwise assume probabilities\n",
    "    if main_output.min() < 0 or main_output.max() > 1:\n",
    "        mask_probs = torch.sigmoid(main_output)\n",
    "    else:\n",
    "        mask_probs = main_output\n",
    "    \n",
    "    # Threshold predictions to binary\n",
    "    predicted = (mask_probs > threshold).float()\n",
    "    \n",
    "    # Calculate IoU efficiently\n",
    "    intersection = (predicted * targets).sum(dim=(1, 2, 3))\n",
    "    predicted_sum = predicted.sum(dim=(1, 2, 3))\n",
    "    gt_sum = targets.sum(dim=(1, 2, 3))\n",
    "    union = predicted_sum + gt_sum - intersection\n",
    "    \n",
    "    # Fast empty check using .any() - both masks are completely empty\n",
    "    both_empty = ~predicted.any(dim=(1, 2, 3)) & ~targets.any(dim=(1, 2, 3))\n",
    "    \n",
    "    # Calculate IoU per image\n",
    "    iou_per_image = intersection / (union + smooth)\n",
    "    \n",
    "    # Set IoU to 0.0 when both masks are empty (authentic images)\n",
    "    # This makes IoU only measure forgery detection performance\n",
    "    iou_per_image = torch.where(both_empty, torch.zeros_like(iou_per_image), iou_per_image)\n",
    "    \n",
    "    # Return mean IoU\n",
    "    return iou_per_image.mean()\n",
    "\n",
    "def calculate_f1_score(predicted, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate pixel-wise F1 score for binary segmentation.\n",
    "    \n",
    "    Args:\n",
    "        predicted: Binary predictions tensor (B, 1, H, W) or (B, H, W) - values in {0, 1}\n",
    "        targets: Ground truth masks tensor (B, 1, H, W) or (B, H, W) - values in {0, 1}\n",
    "        smooth: Smoothing factor to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        F1 score (scalar tensor)\n",
    "    \"\"\"\n",
    "    # Flatten tensors for pixel-wise calculation\n",
    "    predicted_flat = predicted.view(-1)\n",
    "    targets_flat = targets.view(-1)\n",
    "    \n",
    "    # Calculate True Positives, False Positives, False Negatives\n",
    "    tp = (predicted_flat * targets_flat).sum()\n",
    "    fp = (predicted_flat * (1 - targets_flat)).sum()\n",
    "    fn = ((1 - predicted_flat) * targets_flat).sum()\n",
    "    \n",
    "    # Calculate Precision and Recall\n",
    "    precision = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "    \n",
    "    # Calculate F1 score (harmonic mean of precision and recall)\n",
    "    f1 = (2 * precision * recall + smooth) / (precision + recall + smooth)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def calculate_metrics(logits_or_probs, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate both IoU and F1 score for binary segmentation.\n",
    "    \n",
    "    Args:\n",
    "        logits_or_probs: Model output logits (B, 1, H, W) or probabilities (B, 1, H, W)\n",
    "                        Can be a tuple for deep supervision (uses first element)\n",
    "        targets: Ground truth masks tensor (B, 1, H, W) - values in [0, 1]\n",
    "        threshold: Threshold for binarizing predictions (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (iou, f1) scores (both scalar tensors)\n",
    "    \"\"\"\n",
    "    # Handle deep supervision outputs (tuple) or single output (tensor)\n",
    "    if isinstance(logits_or_probs, tuple):\n",
    "        main_output = logits_or_probs[0]  # Use main output for metrics\n",
    "    else:\n",
    "        main_output = logits_or_probs\n",
    "    \n",
    "    # Apply sigmoid if logits, otherwise assume probabilities\n",
    "    if main_output.min() < 0 or main_output.max() > 1:\n",
    "        mask_probs = torch.sigmoid(main_output)\n",
    "    else:\n",
    "        mask_probs = main_output\n",
    "    \n",
    "    # Threshold predictions to binary\n",
    "    predicted = (mask_probs > threshold).float()\n",
    "    \n",
    "    # Calculate IoU (pass original logits/probs, not processed)\n",
    "    iou = calculate_iou(logits_or_probs, targets, threshold=threshold)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = calculate_f1_score(predicted, targets)\n",
    "    \n",
    "    return iou, f1\n",
    "\n",
    "print(\"✓ Loss functions and metrics defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def denormalize_imagenet(img_tensor):\n",
    "    \"\"\"\n",
    "    Denormalize ImageNet-normalized images for correct display.\n",
    "    \n",
    "    Args:\n",
    "        img_tensor: Tensor of shape (B, C, H, W) or (B, H, W, C) or (C, H, W) or numpy array of shape (H, W, C)\n",
    "                    with ImageNet normalization applied\n",
    "    \n",
    "    Returns:\n",
    "        Denormalized image in range [0, 1] ready for display\n",
    "    \"\"\"\n",
    "    # ImageNet mean and std\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Handle different input formats\n",
    "    if isinstance(img_tensor, torch.Tensor):\n",
    "        img_np = img_tensor.cpu().numpy()\n",
    "    else:\n",
    "        img_np = img_tensor.copy()\n",
    "    \n",
    "    # Handle different tensor shapes\n",
    "    if len(img_np.shape) == 4:  # Batch dimension present\n",
    "        # Check if it's (B, C, H, W) or (B, H, W, C)\n",
    "        if img_np.shape[1] == 3:  # (B, C, H, W)\n",
    "            # Permute to (B, H, W, C) for easier broadcasting\n",
    "            img_np = img_np.transpose(0, 2, 3, 1)\n",
    "        # img_np is now (B, H, W, C)\n",
    "        # Denormalize: img = img * std + mean\n",
    "        img_np = img_np * std + mean\n",
    "    elif len(img_np.shape) == 3:\n",
    "        if img_np.shape[0] == 3:  # (C, H, W)\n",
    "            # Permute to (H, W, C)\n",
    "            img_np = img_np.transpose(1, 2, 0)\n",
    "        # img_np is now (H, W, C)\n",
    "        img_np = img_np * std + mean\n",
    "    \n",
    "    # Clip to valid range [0, 1]\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    return img_np\n",
    "\n",
    "def save_visualization(images, masks, predictions, epoch, split='train', viz_dir=None):\n",
    "    \"\"\"\n",
    "    Save visualization images from a batch.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of shape (B, C, H, W) - original images (normalized)\n",
    "        masks: Tensor of shape (B, 1, H, W) - ground truth masks\n",
    "        predictions: Tensor of shape (B, 1, H, W) - predicted masks (probabilities)\n",
    "        epoch: Current epoch number\n",
    "        split: 'train' or 'val'\n",
    "        viz_dir: Directory to save visualizations (uses config.VIZ_DIR if None)\n",
    "    \"\"\"\n",
    "    if viz_dir is None:\n",
    "        viz_dir = config.VIZ_DIR\n",
    "    \n",
    "    # Convert tensors to numpy and move to CPU\n",
    "    images_np = images.cpu().permute(0, 2, 3, 1).float().numpy()  # (B, H, W, C)\n",
    "    masks_np = masks.cpu().squeeze(1).float().numpy()  # (B, H, W)\n",
    "    preds_np = predictions.cpu().squeeze(1).float().numpy()  # (B, H, W)\n",
    "    \n",
    "    # Denormalize images for correct display\n",
    "    images_np = denormalize_imagenet(images_np)\n",
    "    \n",
    "    # Threshold predictions to binary\n",
    "    preds_binary = (preds_np > config.PREDICTION_THRESHOLD).astype(np.float32)\n",
    "    \n",
    "    # Save all images from the batch\n",
    "    batch_size = masks_np.shape[0]\n",
    "    for img_idx in range(batch_size):\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "        \n",
    "        # Determine image type\n",
    "        is_forged = masks_np[img_idx].any()\n",
    "        image_type = \"Forged\" if is_forged else \"Original/Authentic\"\n",
    "        \n",
    "        # Original image\n",
    "        img = images_np[img_idx]\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(f'{image_type} Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        axes[1].imshow(masks_np[img_idx], cmap='gray')\n",
    "        axes[1].set_title('Ground Truth Mask')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Predicted mask\n",
    "        axes[2].imshow(preds_binary[img_idx], cmap='gray')\n",
    "        axes[2].set_title('Predicted Mask')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Overlay: original image with predicted mask in red\n",
    "        overlay = img.copy()\n",
    "        mask_colored = np.zeros_like(overlay)\n",
    "        mask_colored[:, :, 0] = preds_binary[img_idx]  # Red channel\n",
    "        overlay = np.clip(overlay * 0.6 + mask_colored * 0.4, 0, 1)\n",
    "        axes[3].imshow(overlay)\n",
    "        axes[3].set_title('Overlay (Original + Prediction)')\n",
    "        axes[3].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(viz_dir, f'{split}_epoch_{epoch:03d}_{img_idx+1}.jpg')\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight', format='jpg')\n",
    "        plt.close()\n",
    "\n",
    "print(\"✓ Visualization functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1751816057043,
     "user": {
      "displayName": "JAYESH SHARMA",
      "userId": "10181368842352240411"
     },
     "user_tz": -330
    },
    "id": "4r4rl96VQSfh",
    "outputId": "dd80a34c-d22f-4309-9dd1-d8956068fc0a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class JointTransform:\n",
    "    \"\"\"Apply the same spatial transformations to both image and mask\"\"\"\n",
    "    def __init__(self, rotation=15, hflip_prob=0.5, vflip_prob=0.0, \n",
    "                 enable_rotation=True, enable_hflip=True, enable_vflip=True):\n",
    "        self.rotation = rotation if enable_rotation else 0\n",
    "        self.hflip_prob = hflip_prob if enable_hflip else 0.0\n",
    "        self.vflip_prob = vflip_prob if enable_vflip else 0.0\n",
    "        # Store last transformation parameters for weight_map\n",
    "        self.last_angle = 0\n",
    "        self.last_hflip = False\n",
    "        self.last_vflip = False\n",
    "    \n",
    "    def __call__(self, img, mask):\n",
    "        # Random rotation\n",
    "        if self.rotation > 0:\n",
    "            angle = random.uniform(-self.rotation, self.rotation)\n",
    "            self.last_angle = angle\n",
    "            img = TF.rotate(img, angle, interpolation=TF.InterpolationMode.BILINEAR, fill=0)\n",
    "            mask = TF.rotate(mask, angle, interpolation=TF.InterpolationMode.NEAREST, fill=0)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        self.last_hflip = False\n",
    "        if self.hflip_prob > 0 and random.random() < self.hflip_prob:\n",
    "            self.last_hflip = True\n",
    "            img = TF.hflip(img)\n",
    "            mask = TF.hflip(mask)\n",
    "        \n",
    "        # Random vertical flip\n",
    "        self.last_vflip = False\n",
    "        if self.vflip_prob > 0 and random.random() < self.vflip_prob:\n",
    "            self.last_vflip = True\n",
    "            img = TF.vflip(img)\n",
    "            mask = TF.vflip(mask)\n",
    "        \n",
    "        return img, mask\n",
    "    \n",
    "    def transform_mask_only(self, mask):\n",
    "        \"\"\"Apply the same transformations to weight_map that were applied to img/mask\"\"\"\n",
    "        # Apply rotation\n",
    "        if self.rotation > 0 and self.last_angle != 0:\n",
    "            mask = TF.rotate(mask, self.last_angle, interpolation=TF.InterpolationMode.NEAREST, fill=1.0)\n",
    "        \n",
    "        # Apply horizontal flip\n",
    "        if self.last_hflip:\n",
    "            mask = TF.hflip(mask)\n",
    "        \n",
    "        # Apply vertical flip\n",
    "        if self.last_vflip:\n",
    "            mask = TF.vflip(mask)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "def apply_size_based_weighting(weight_map, mask, min_area=0.01, max_area=0.10, min_weight=1.0, max_weight=10.0):\n",
    "    \"\"\"\n",
    "    Apply linear inverse size-based weighting to weight map.\n",
    "    \n",
    "    Args:\n",
    "        weight_map: Weight map to modify (numpy array)\n",
    "        mask: Binary mask (numpy array, values in [0, 1])\n",
    "        min_area: Minimum area ratio (1% = 0.01) - gets max_weight\n",
    "        max_area: Maximum area ratio (10% = 0.10) - gets min_weight\n",
    "        min_weight: Weight for forgeries >= max_area\n",
    "        max_weight: Weight for forgeries <= min_area\n",
    "    \n",
    "    Returns:\n",
    "        Modified weight_map with size-based weights applied\n",
    "    \"\"\"\n",
    "    if mask.max() == 0:\n",
    "        return weight_map  # No forgeries, return unchanged\n",
    "    \n",
    "    mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_uint8, connectivity=8)\n",
    "    \n",
    "    total_pixels = mask.shape[0] * mask.shape[1]\n",
    "    \n",
    "    # For each connected component (forgery region)\n",
    "    for label_id in range(1, num_labels):\n",
    "        area = stats[label_id, cv2.CC_STAT_AREA]\n",
    "        area_ratio = area / total_pixels\n",
    "        \n",
    "        # Calculate size-based weight using linear interpolation\n",
    "        if area_ratio <= min_area:\n",
    "            # Very small forgeries (<= 1%): maximum weight\n",
    "            size_weight = max_weight\n",
    "        elif area_ratio >= max_area:\n",
    "            # Large forgeries (>= 10%): minimum weight\n",
    "            size_weight = min_weight\n",
    "        else:\n",
    "            # Linear interpolation between min_area and max_area\n",
    "            # weight = max_weight - (max_weight - min_weight) * (area_ratio - min_area) / (max_area - min_area)\n",
    "            normalized = (area_ratio - min_area) / (max_area - min_area)\n",
    "            size_weight = max_weight - (max_weight - min_weight) * normalized\n",
    "        \n",
    "        # Apply to weight map for this component (multiply existing weights)\n",
    "        component_mask = (labels == label_id)\n",
    "        weight_map[component_mask] *= size_weight\n",
    "    \n",
    "    return weight_map\n",
    "\n",
    "class SyntheticCopyMoveForgery:\n",
    "    \"\"\"Apply synthetic copy-move forgeries to authentic images using grid-based placement\"\"\"\n",
    "    def __init__(self, prob=0.5, min_regions=1, max_regions=3, \n",
    "                 min_copies=1, max_copies=3, min_width=35, max_width=100,\n",
    "                 min_aspect=1.0, max_aspect=4.0, min_scale=0.8, max_scale=1.2, \n",
    "                 origin_weight=3.0, probe_weight=1.0, grid_size=64, blur_border=False,\n",
    "                 origin_rotation=True, origin_rotation_range=90,\n",
    "                 probe_rotation=True, probe_rotation_range=90):\n",
    "        self.prob = prob\n",
    "        self.min_regions = min_regions\n",
    "        self.max_regions = max_regions\n",
    "        self.min_copies = min_copies\n",
    "        self.max_copies = max_copies\n",
    "        self.min_width = min_width\n",
    "        self.max_width = max_width\n",
    "        self.min_aspect = min_aspect\n",
    "        self.max_aspect = max_aspect\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.origin_weight = origin_weight\n",
    "        self.probe_weight = probe_weight\n",
    "        self.grid_size = grid_size\n",
    "        self.blur_border = blur_border\n",
    "        self.origin_rotation = origin_rotation\n",
    "        self.origin_rotation_range = origin_rotation_range\n",
    "        self.probe_rotation = probe_rotation\n",
    "        self.probe_rotation_range = probe_rotation_range\n",
    "    \n",
    "    def _create_grid_mask(self, img_height, img_width):\n",
    "        \"\"\"Create a grid mask to track occupied cells\"\"\"\n",
    "        grid_h = (img_height + self.grid_size - 1) // self.grid_size\n",
    "        grid_w = (img_width + self.grid_size - 1) // self.grid_size\n",
    "        return np.zeros((grid_h, grid_w), dtype=np.uint8)\n",
    "    \n",
    "    def _mark_grid_cells(self, grid_mask, x, y, w, h, img_height, img_width, margin=5):\n",
    "        \"\"\"Mark grid cells as occupied\"\"\"\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(img_width, x + w + margin)\n",
    "        y2 = min(img_height, y + h + margin)\n",
    "        grid_x1 = x1 // self.grid_size\n",
    "        grid_y1 = y1 // self.grid_size\n",
    "        grid_x2 = (x2 + self.grid_size - 1) // self.grid_size\n",
    "        grid_y2 = (y2 + self.grid_size - 1) // self.grid_size\n",
    "        grid_mask[grid_y1:grid_y2, grid_x1:grid_x2] = 1\n",
    "    \n",
    "    def _check_grid_cells(self, grid_mask, x, y, w, h, img_height, img_width, margin=5):\n",
    "        \"\"\"Check if grid cells are available for a region\"\"\"\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(img_width, x + w + margin)\n",
    "        y2 = min(img_height, y + h + margin)\n",
    "        grid_x1 = x1 // self.grid_size\n",
    "        grid_y1 = y1 // self.grid_size\n",
    "        grid_x2 = (x2 + self.grid_size - 1) // self.grid_size\n",
    "        grid_y2 = (y2 + self.grid_size - 1) // self.grid_size\n",
    "        return grid_mask[grid_y1:grid_y2, grid_x1:grid_x2].max() == 0\n",
    "    \n",
    "    def _get_random_region_size(self, img_height, img_width, region_type):\n",
    "        \"\"\"Generate random size for a region\"\"\"\n",
    "        width = random.randint(self.min_width, min(self.max_width, img_width // 3))\n",
    "        aspect_ratio = random.uniform(self.min_aspect, self.max_aspect)\n",
    "        if random.random() < 0.5:\n",
    "            height = int(width * aspect_ratio)\n",
    "        else:\n",
    "            height = int(width / aspect_ratio)\n",
    "        height = max(self.min_width, min(height, img_height // 3))\n",
    "        return width, height\n",
    "    \n",
    "    def _get_random_position(self, img_height, img_width, width, height, grid_mask, margin=5):\n",
    "        \"\"\"Get a random position that doesn't overlap with occupied grid cells\"\"\"\n",
    "        max_attempts = 50\n",
    "        for _ in range(max_attempts):\n",
    "            x = random.randint(margin, img_width - width - margin)\n",
    "            y = random.randint(margin, img_height - height - margin)\n",
    "            if self._check_grid_cells(grid_mask, x, y, width, height, img_height, img_width, margin):\n",
    "                return x, y\n",
    "        return None, None\n",
    "    \n",
    "    def _create_region_mask(self, img_height, img_width, width, height, x, y, region_type):\n",
    "        \"\"\"Create a binary mask for a specific region type\"\"\"\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "        if region_type == 'circle':\n",
    "            radius = min(width, height) // 2\n",
    "            center_x = x + width // 2\n",
    "            center_y = y + height // 2\n",
    "            cv2.circle(mask, (center_x, center_y), radius, 255, -1)\n",
    "        elif region_type == 'oval':\n",
    "            center_x = x + width // 2\n",
    "            center_y = y + height // 2\n",
    "            axes = (width // 2, height // 2)\n",
    "            cv2.ellipse(mask, (center_x, center_y), axes, 0, 0, 360, 255, -1)\n",
    "        elif region_type == 'rectangle':\n",
    "            cv2.rectangle(mask, (x, y), (x + width, y + height), 255, -1)\n",
    "        return mask\n",
    "    \n",
    "    def _rotate_region(self, region, region_mask, angle):\n",
    "        \"\"\"Rotate a region and its mask using BORDER_REPLICATE to avoid artifacts\"\"\"\n",
    "        if abs(angle) < 0.1:\n",
    "            return region, region_mask\n",
    "        h, w = region.shape[:2]\n",
    "        center = (w / 2.0, h / 2.0)\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        cos = np.abs(rotation_matrix[0, 0])\n",
    "        sin = np.abs(rotation_matrix[0, 1])\n",
    "        new_w = int((h * sin) + (w * cos))\n",
    "        new_h = int((h * cos) + (w * sin))\n",
    "        rotation_matrix[0, 2] += (new_w / 2) - center[0]\n",
    "        rotation_matrix[1, 2] += (new_h / 2) - center[1]\n",
    "        region_rotated = cv2.warpAffine(region, rotation_matrix, (new_w, new_h), \n",
    "                                        flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "        region_mask_rotated = cv2.warpAffine(region_mask, rotation_matrix, (new_w, new_h),\n",
    "                                             flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "        return region_rotated, region_mask_rotated\n",
    "    \n",
    "    def _apply_copy(self, img, mask, source_mask, target_x, target_y, scale_factor=1.0, \n",
    "                    region_type=None, width=None, height=None, origin_rotation=0.0, probe_rotation=0.0):\n",
    "        \"\"\"Copy a region from source to target with alpha blending insertion\"\"\"\n",
    "        contours, _ = cv2.findContours(source_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "            return img, mask, False\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "        region = img[y:y+h, x:x+w].copy()\n",
    "        region_mask = source_mask[y:y+h, x:x+w].copy()\n",
    "        region_mask = (region_mask > 127).astype(np.uint8) * 255\n",
    "        if abs(origin_rotation) > 0.1:\n",
    "            region, region_mask = self._rotate_region(region, region_mask, origin_rotation)\n",
    "            h, w = region.shape[:2]\n",
    "        if abs(scale_factor - 1.0) > 0.01:\n",
    "            new_w = max(1, int(w * scale_factor))\n",
    "            new_h = max(1, int(h * scale_factor))\n",
    "            region = cv2.resize(region, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "            region_mask = cv2.resize(region_mask, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "            w, h = new_w, new_h\n",
    "        if abs(probe_rotation) > 0.1:\n",
    "            region, region_mask = self._rotate_region(region, region_mask, probe_rotation)\n",
    "            h, w = region.shape[:2]\n",
    "        if target_x < 0 or target_y < 0 or target_x + w > img.shape[1] or target_y + h > img.shape[0]:\n",
    "            return img, mask, False\n",
    "        alpha = region_mask.astype(np.float32) / 255.0\n",
    "        alpha[alpha < 0.05] = 0.0\n",
    "        alpha_3d = alpha[:, :, np.newaxis]\n",
    "        target_patch = img[target_y:target_y+h, target_x:target_x+w]\n",
    "        img[target_y:target_y+h, target_x:target_x+w] = alpha_3d * region + (1.0 - alpha_3d) * target_patch\n",
    "        if self.blur_border and alpha.max() > 0:\n",
    "            patch = img[target_y:target_y+h, target_x:target_x+w].copy()\n",
    "            binary_mask = (alpha > 0.5).astype(np.uint8)\n",
    "            dist = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 3)\n",
    "            blur_zone = ((dist > 0) & (dist <= 3.0)).astype(np.float32)[:, :, np.newaxis]\n",
    "            blurred = cv2.GaussianBlur(patch, (5, 5), 1.0)\n",
    "            img[target_y:target_y+h, target_x:target_x+w] = blur_zone * blurred + (1.0 - blur_zone) * patch\n",
    "        probe_mask_uint8 = ((alpha > 0.5).astype(np.uint8)) * 255\n",
    "        mask[target_y:target_y+h, target_x:target_x+w] = np.maximum(\n",
    "            mask[target_y:target_y+h, target_x:target_x+w], probe_mask_uint8)\n",
    "        return img, mask, True\n",
    "    \n",
    "    def __call__(self, img, mask):\n",
    "        \"\"\"Apply synthetic copy-move forgeries to image and update mask using grid-based placement\"\"\"\n",
    "        if mask.max() > 0:\n",
    "            weight_map = np.ones_like(mask, dtype=np.float32)\n",
    "            return img, mask, weight_map\n",
    "        if random.random() > self.prob:\n",
    "            weight_map = np.ones_like(mask, dtype=np.float32)\n",
    "            return img, mask, weight_map\n",
    "        img_height, img_width = img.shape[:2]\n",
    "        mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "        weight_map = np.ones((img_height, img_width), dtype=np.float32)\n",
    "        grid_mask = self._create_grid_mask(img_height, img_width)\n",
    "        num_regions = random.randint(self.min_regions, self.max_regions)\n",
    "        region_types = ['circle', 'oval', 'rectangle']\n",
    "        successful_forgeries = []\n",
    "        for region_idx in range(num_regions):\n",
    "            region_type = random.choice(region_types)\n",
    "            width, height = self._get_random_region_size(img_height, img_width, region_type)\n",
    "            source_x, source_y = self._get_random_position(img_height, img_width, width, height, grid_mask, margin=5)\n",
    "            if source_x is None or source_y is None:\n",
    "                continue\n",
    "            source_mask = self._create_region_mask(img_height, img_width, width, height, source_x, source_y, region_type)\n",
    "            num_copies = random.randint(self.min_copies, self.max_copies)\n",
    "            placed_copies = []\n",
    "            temp_grid_mask = grid_mask.copy()\n",
    "            self._mark_grid_cells(temp_grid_mask, source_x, source_y, width, height, img_height, img_width, margin=5)\n",
    "            for copy_idx in range(num_copies):\n",
    "                scale_factor = random.uniform(self.min_scale, self.max_scale)\n",
    "                origin_rot = 0.0\n",
    "                probe_rot = 0.0\n",
    "                if self.origin_rotation:\n",
    "                    origin_rot = random.uniform(-self.origin_rotation_range, self.origin_rotation_range)\n",
    "                if self.probe_rotation:\n",
    "                    probe_rot = random.uniform(-self.probe_rotation_range, self.probe_rotation_range)\n",
    "                max_rotation = max(abs(origin_rot), abs(probe_rot)) if (abs(origin_rot) > 0.1 or abs(probe_rot) > 0.1) else 0\n",
    "                if max_rotation > 0.1:\n",
    "                    rotation_factor = 1.5 if max_rotation > 45 else 1.2\n",
    "                    scaled_width = int(width * scale_factor * rotation_factor)\n",
    "                    scaled_height = int(height * scale_factor * rotation_factor)\n",
    "                else:\n",
    "                    scaled_width = int(width * scale_factor)\n",
    "                    scaled_height = int(height * scale_factor)\n",
    "                for attempt in range(50):\n",
    "                    target_x, target_y = self._get_random_position(img_height, img_width, scaled_width, scaled_height, temp_grid_mask, margin=5)\n",
    "                    if target_x is None or target_y is None:\n",
    "                        continue\n",
    "                    distance = np.sqrt((target_x - source_x)**2 + (target_y - source_y)**2)\n",
    "                    if distance < 30:\n",
    "                        continue\n",
    "                    placed_copies.append((target_x, target_y, scale_factor, origin_rot, probe_rot))\n",
    "                    self._mark_grid_cells(temp_grid_mask, target_x, target_y, scaled_width, scaled_height, img_height, img_width, margin=5)\n",
    "                    break\n",
    "            if len(placed_copies) > 0:\n",
    "                successful_forgeries.append({\n",
    "                    'source_mask': source_mask, 'source_x': source_x, 'source_y': source_y,\n",
    "                    'width': width, 'height': height, 'copies': placed_copies, 'region_type': region_type})\n",
    "                grid_mask = temp_grid_mask\n",
    "        for forgery in successful_forgeries:\n",
    "            source_mask = forgery['source_mask']\n",
    "            copies_applied = 0\n",
    "            for copy_info in forgery['copies']:\n",
    "                if len(copy_info) == 5:\n",
    "                    target_x, target_y, scale_factor, origin_rot, probe_rot = copy_info\n",
    "                else:\n",
    "                    target_x, target_y, scale_factor = copy_info\n",
    "                    origin_rot, probe_rot = 0.0, 0.0\n",
    "                img, mask_uint8, success = self._apply_copy(img, mask_uint8, source_mask, target_x, target_y, scale_factor,\n",
    "                    region_type=forgery['region_type'], width=forgery['width'], height=forgery['height'],\n",
    "                    origin_rotation=origin_rot, probe_rotation=probe_rot)\n",
    "                if success:\n",
    "                    copies_applied += 1\n",
    "            if copies_applied > 0:\n",
    "                mask_uint8 = np.maximum(mask_uint8, source_mask)\n",
    "                weight_map = np.where(source_mask > 0, self.origin_weight, weight_map)\n",
    "                probe_mask = (mask_uint8 > 0) & (source_mask == 0)\n",
    "                weight_map = np.where(probe_mask, self.probe_weight, weight_map)\n",
    "        mask = mask_uint8.astype(np.float32) / 255.0\n",
    "        return img, mask, weight_map\n",
    "\n",
    "class UnifiedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unified dataset loader for combined_dataset structure.\n",
    "    Supports both old structure (subfolder/images/, subfolder/masks/) and \n",
    "    new structure (subfolder/train/images/, subfolder/test/images/).\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_root, img_size=(512, 512), transform=None, joint_transform=None, \n",
    "                 mask_blur_radius=0.0, split='train', datasets_to_load=None, datasets_to_skip=None,\n",
    "                 filter_by_mask_area=False, min_mask_area_percent=0.01, max_mask_area_percent=0.10, synthetic_forgery=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_root: Root folder containing subfolders (e.g., 'combined_dataset')\n",
    "                         Each subfolder should have either:\n",
    "                         - Old: images/ (PNG) and masks/ (NPY) folders\n",
    "                         - New: train/ and test/ subfolders, each with images/ and masks/\n",
    "            img_size: Target size for resizing images and masks (height, width)\n",
    "            transform: Image-only transforms (brightness, contrast, etc.)\n",
    "            joint_transform: Spatial transforms applied to both image and mask\n",
    "            mask_blur_radius: Gaussian blur radius for mask label smoothing (0.0 = disabled)\n",
    "            split: 'train', 'test', or 'all' - which split to load (default: 'train')\n",
    "            datasets_to_load: List of dataset subfolder names to include (None = all)\n",
    "            datasets_to_skip: List of dataset subfolder names to exclude (applied after datasets_to_load)\n",
    "            filter_by_mask_area: Enable mask area filtering (default: False)\n",
    "            min_mask_area_percent: Minimum mask area as fraction (0.01 = 1%, default: 0.01)\n",
    "            max_mask_area_percent: Maximum mask area as fraction (0.10 = 10%, default: 0.10)\n",
    "        \"\"\"\n",
    "        self.dataset_root = Path(dataset_root)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.joint_transform = joint_transform\n",
    "        self.mask_blur_radius = mask_blur_radius\n",
    "        self.synthetic_forgery = synthetic_forgery\n",
    "        self.split = split\n",
    "        self.datasets_to_load = datasets_to_load\n",
    "        self.datasets_to_skip = datasets_to_skip or []\n",
    "        \n",
    "        # Mask area filtering parameters\n",
    "        self.filter_by_mask_area = filter_by_mask_area\n",
    "        self.min_mask_area_percent = min_mask_area_percent\n",
    "        self.max_mask_area_percent = max_mask_area_percent\n",
    "        \n",
    "        # Find all image-mask pairs from all subfolders\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        self.loaded_datasets = []  # Track which datasets were actually loaded\n",
    "        self.filtered_out_count = 0  # Track how many pairs were filtered out\n",
    "        \n",
    "        # Iterate through all subfolders in dataset_root\n",
    "        for subfolder in self.dataset_root.iterdir():\n",
    "            if not subfolder.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Get subfolder name for filtering\n",
    "            subfolder_name = subfolder.name\n",
    "            \n",
    "            # Apply dataset filtering\n",
    "            # First check if we should load this dataset\n",
    "            if self.datasets_to_load is not None:\n",
    "                if subfolder_name not in self.datasets_to_load:\n",
    "                    continue  # Skip if not in the list\n",
    "            \n",
    "            # Then check if we should skip this dataset\n",
    "            if subfolder_name in self.datasets_to_skip:\n",
    "                continue  # Skip if in the skip list\n",
    "            \n",
    "            # Track that we're loading this dataset\n",
    "            if subfolder_name not in self.loaded_datasets:\n",
    "                self.loaded_datasets.append(subfolder_name)\n",
    "            \n",
    "            # Check for new structure (train/test subfolders)\n",
    "            if (subfolder / 'train').exists() or (subfolder / 'test').exists():\n",
    "                # New structure: subfolder/train/ and subfolder/test/\n",
    "                splits_to_load = []\n",
    "                if split == 'train':\n",
    "                    splits_to_load = ['train']\n",
    "                elif split == 'test':\n",
    "                    splits_to_load = ['test']\n",
    "                elif split == 'all':\n",
    "                    splits_to_load = ['train', 'test']\n",
    "                \n",
    "                for split_name in splits_to_load:\n",
    "                    split_dir = subfolder / split_name\n",
    "                    if not split_dir.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    images_dir = split_dir / 'images'\n",
    "                    masks_dir = split_dir / 'masks'\n",
    "                    \n",
    "                    if not images_dir.exists() or not masks_dir.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    # Find all PNG images in images/ folder\n",
    "                    for img_path in images_dir.glob('*.png'):\n",
    "                        # Find corresponding mask (same filename but .npy extension)\n",
    "                        mask_path = masks_dir / f\"{img_path.stem}.npy\"\n",
    "                        \n",
    "                        if mask_path.exists():\n",
    "                            # Check if pair should be included based on mask area filtering\n",
    "                            if self._should_include_pair(mask_path):\n",
    "                                self.image_paths.append(str(img_path))\n",
    "                                self.mask_paths.append(str(mask_path))\n",
    "                            else:\n",
    "                                self.filtered_out_count += 1\n",
    "            else:\n",
    "                # Old structure: subfolder/images/ and subfolder/masks/\n",
    "                images_dir = subfolder / 'images'\n",
    "                masks_dir = subfolder / 'masks'\n",
    "                \n",
    "                if images_dir.exists() and masks_dir.exists():\n",
    "                    # Find all PNG images in images/ folder\n",
    "                    for img_path in images_dir.glob('*.png'):\n",
    "                        # Find corresponding mask (same filename but .npy extension)\n",
    "                        mask_path = masks_dir / f\"{img_path.stem}.npy\"\n",
    "                        \n",
    "                        if mask_path.exists():\n",
    "                            # Check if pair should be included based on mask area filtering\n",
    "                            if self._should_include_pair(mask_path):\n",
    "                                self.image_paths.append(str(img_path))\n",
    "                                self.mask_paths.append(str(mask_path))\n",
    "                            else:\n",
    "                                self.filtered_out_count += 1\n",
    "        \n",
    "        # Sort for consistent ordering\n",
    "        combined = list(zip(self.image_paths, self.mask_paths))\n",
    "        combined.sort()\n",
    "        self.image_paths, self.mask_paths = zip(*combined)\n",
    "        self.image_paths = list(self.image_paths)\n",
    "        self.mask_paths = list(self.mask_paths)\n",
    "        \n",
    "        num_subfolders = len(set(Path(p).parent.parent.parent if 'train' in str(p) or 'test' in str(p) else Path(p).parent.parent for p in self.image_paths))\n",
    "        \n",
    "        # Print dataset loading information\n",
    "        if self.loaded_datasets:\n",
    "            datasets_str = ', '.join(sorted(self.loaded_datasets))\n",
    "            print(f\"Found {len(self.image_paths)} image-mask pairs from {num_subfolders} subfolders (split: {split})\")\n",
    "            print(f\"  Loaded datasets: {datasets_str}\")\n",
    "            if self.datasets_to_load is not None:\n",
    "                print(f\"  (Filtered to: {self.datasets_to_load})\")\n",
    "            if self.datasets_to_skip:\n",
    "                print(f\"  (Excluded: {self.datasets_to_skip})\")\n",
    "            if self.filter_by_mask_area:\n",
    "                print(f\"  Mask area filter: {self.min_mask_area_percent*100:.1f}% - {self.max_mask_area_percent*100:.1f}%\")\n",
    "                if self.filtered_out_count > 0:\n",
    "                    print(f\"  Filtered out: {self.filtered_out_count} pairs (outside area range)\")\n",
    "        else:\n",
    "            print(f\"⚠ Warning: No datasets loaded! Check DATASETS_TO_LOAD and DATASETS_TO_SKIP settings.\")\n",
    "            print(f\"  Available subfolders: {[d.name for d in self.dataset_root.iterdir() if d.is_dir()]}\")\n",
    "    \n",
    "    def _calculate_mask_area_percent(self, mask_path):\n",
    "        \"\"\"\n",
    "        Calculate mask area as percentage of image area.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to .npy mask file\n",
    "        \n",
    "        Returns:\n",
    "            Mask area percentage (0.0 to 1.0, where 0.01 = 1%)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load mask from .npy file\n",
    "            mask = np.load(mask_path)\n",
    "            \n",
    "            # Handle multi-channel masks: OR all channels together\n",
    "            if len(mask.shape) > 2:\n",
    "                mask = np.any(mask > 0, axis=0).astype(np.uint8)\n",
    "            else:\n",
    "                # Single channel mask - convert to binary\n",
    "                if mask.dtype != np.uint8:\n",
    "                    if mask.max() <= 1.0:\n",
    "                        mask = (mask > 0).astype(np.uint8)\n",
    "                    else:\n",
    "                        mask = (mask > 0).astype(np.uint8)\n",
    "                else:\n",
    "                    mask = (mask > 0).astype(np.uint8)\n",
    "            \n",
    "            # Calculate area percentage\n",
    "            total_pixels = mask.size\n",
    "            mask_pixels = mask.sum()\n",
    "            area_percent = mask_pixels / total_pixels if total_pixels > 0 else 0.0\n",
    "            \n",
    "            return area_percent\n",
    "        except Exception as e:\n",
    "            # If mask loading fails, return 0 (will be treated as authentic/empty)\n",
    "            return 0.0\n",
    "    \n",
    "    def _should_include_pair(self, mask_path):\n",
    "        \"\"\"\n",
    "        Check if image-mask pair should be included based on mask area filtering.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to mask file\n",
    "        \n",
    "        Returns:\n",
    "            True if pair should be included, False otherwise\n",
    "        \"\"\"\n",
    "        if not self.filter_by_mask_area:\n",
    "            return True  # No filtering, include all\n",
    "        \n",
    "        # Calculate mask area percentage\n",
    "        area_percent = self._calculate_mask_area_percent(mask_path)\n",
    "        \n",
    "        # Authentic images (empty masks) are always included\n",
    "        if area_percent == 0.0:\n",
    "            return True\n",
    "        \n",
    "        # Check if area is within the specified range\n",
    "        return self.min_mask_area_percent <= area_percent <= self.max_mask_area_percent\n",
    "    \n",
    "    def _apply_mask_blur(self, mask):\n",
    "        \"\"\"Apply Gaussian blur to mask if blur radius > 0\"\"\"\n",
    "        if self.mask_blur_radius > 0:\n",
    "            import math\n",
    "            kernel_size = int(2 * math.ceil(3 * self.mask_blur_radius) + 1)\n",
    "            if kernel_size % 2 == 0:\n",
    "                kernel_size += 1\n",
    "            mask = cv2.GaussianBlur(mask, (kernel_size, kernel_size), self.mask_blur_radius)\n",
    "        return mask\n",
    "    \n",
    "    def _load_and_process_mask(self, mask_path):\n",
    "        \"\"\"\n",
    "        Load mask from .npy file, handle multi-channel by ORing, resize to target size.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to .npy mask file\n",
    "        \n",
    "        Returns:\n",
    "            Processed mask as numpy array (H, W) in range [0, 1]\n",
    "        \"\"\"\n",
    "        # Load mask from .npy file\n",
    "        mask = np.load(mask_path)\n",
    "        \n",
    "        # Handle multi-channel masks: OR all channels together\n",
    "        if len(mask.shape) > 2:\n",
    "            # OR operation across channels: if any channel has a pixel > 0, result is 1\n",
    "            mask = np.any(mask > 0, axis=0).astype(np.uint8) * 255\n",
    "        else:\n",
    "            # Single channel mask\n",
    "            if mask.dtype != np.uint8:\n",
    "                # Normalize to uint8 if needed\n",
    "                if mask.max() <= 1.0:\n",
    "                    mask = (mask * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    mask = mask.astype(np.uint8)\n",
    "        \n",
    "        # Resize to target size\n",
    "        mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Apply Gaussian blur for label smoothing if enabled\n",
    "        mask = self._apply_mask_blur(mask)\n",
    "        \n",
    "        # Normalize to [0, 1] range\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img = cv2.imread(self.image_paths[idx], cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {self.image_paths[idx]}\")\n",
    "        \n",
    "        # Handle different image formats (RGB, grayscale, etc.)\n",
    "        if len(img.shape) == 2:\n",
    "            # Grayscale image, convert to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        elif img.shape[2] == 4:\n",
    "            # RGBA image, convert to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n",
    "        else:\n",
    "            # BGR image, convert to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image to target size\n",
    "        # Use INTER_AREA for better quality when downsampling (reduces blur)\n",
    "        img = cv2.resize(img, self.img_size, interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Load and process mask\n",
    "        mask = self._load_and_process_mask(self.mask_paths[idx])\n",
    "        \n",
    "        # Initialize weight map (default: all 1.0)\n",
    "        weight_map = np.ones_like(mask, dtype=np.float32)\n",
    "        \n",
    "        # Apply synthetic copy-move forgery augmentation (before converting to tensors)\n",
    "        # This only affects authentic images with empty masks\n",
    "        # Returns: img, mask, weight_map\n",
    "        if self.synthetic_forgery is not None:\n",
    "            img, mask, weight_map = self.synthetic_forgery(img, mask)\n",
    "        \n",
    "        # Apply size-based weighting if enabled\n",
    "        if config.USE_SIZE_BASED_WEIGHTING:\n",
    "            weight_map = apply_size_based_weighting(\n",
    "                weight_map, mask,\n",
    "                min_area=config.SIZE_WEIGHT_MIN_AREA,\n",
    "                max_area=config.SIZE_WEIGHT_MAX_AREA,\n",
    "                min_weight=config.SIZE_WEIGHT_MIN,\n",
    "                max_weight=config.SIZE_WEIGHT_MAX\n",
    "            )\n",
    "        \n",
    "        # Convert to PyTorch tensors: (H, W, C) -> (C, H, W)\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)  # (3, H, W)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)  # (1, H, W)\n",
    "        weight_map = torch.from_numpy(weight_map).unsqueeze(0)  # (1, H, W)\n",
    "        \n",
    "        # Apply joint spatial transformations (rotation, flip) to both image and mask\n",
    "        if self.joint_transform:\n",
    "            img, mask = self.joint_transform(img, mask)\n",
    "            # Note: weight_map should follow mask transformations\n",
    "            weight_map = self.joint_transform.transform_mask_only(weight_map)\n",
    "        \n",
    "        # Apply image-only transformations (brightness, contrast, saturation)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Normalization logic:\n",
    "        # 1. If USE_PER_IMAGE_MINMAX=True, apply min-max normalization first\n",
    "        # 2. Then apply either per-image z-score OR ImageNet normalization based on USE_PER_IMAGE_ZSCORE\n",
    "        \n",
    "        # Step 1: Per-image min-max normalization (optional)\n",
    "        if config.USE_PER_IMAGE_MINMAX:\n",
    "            # Image is in shape (C, H, W) and range [0, 1]\n",
    "            # Flatten spatial dimensions for percentile calculation: (C, H*W)\n",
    "            img_flat = img.view(3, -1)  # (3, H*W)\n",
    "            \n",
    "            if config.PER_IMAGE_PERCENTILE_CLIP:\n",
    "                # Use percentile clipping to remove 1% extremes (robust to outliers)\n",
    "                # Clips bottom 1% and top 1% (1st to 99th percentile)\n",
    "                lower_percentile = config.PER_IMAGE_LOWER_PERCENTILE  # 1%\n",
    "                upper_percentile = config.PER_IMAGE_UPPER_PERCENTILE  # 99%\n",
    "                \n",
    "                # Compute percentiles for each channel\n",
    "                k_lower = max(1, int(lower_percentile / 100.0 * img_flat.shape[1]))\n",
    "                k_upper = max(1, int(upper_percentile / 100.0 * img_flat.shape[1]))\n",
    "                \n",
    "                # Get percentile values per channel\n",
    "                img_sorted, _ = torch.sort(img_flat, dim=1)\n",
    "                min_vals = img_sorted[:, k_lower - 1].unsqueeze(1)  # (3, 1) - 1st percentile\n",
    "                max_vals = img_sorted[:, k_upper - 1].unsqueeze(1)  # (3, 1) - 99th percentile\n",
    "            else:\n",
    "                # Use actual min/max per channel (no clipping)\n",
    "                min_vals = img_flat.min(dim=1, keepdim=True)[0]  # (3, 1)\n",
    "                max_vals = img_flat.max(dim=1, keepdim=True)[0]  # (3, 1)\n",
    "            \n",
    "            # Reshape for broadcasting: (3, 1, 1)\n",
    "            min_vals = min_vals.view(3, 1, 1)\n",
    "            max_vals = max_vals.view(3, 1, 1)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            range_vals = max_vals - min_vals\n",
    "            range_vals = torch.clamp(range_vals, min=1e-6)\n",
    "            \n",
    "            # Min-max normalization: (img - min) / (max - min) -> [0, 1]\n",
    "            img = (img - min_vals) / range_vals\n",
    "            \n",
    "            # Clip to [0, 1] to handle any numerical issues\n",
    "            img = torch.clamp(img, 0.0, 1.0)\n",
    "        \n",
    "        # Step 2: Apply either per-image z-score OR ImageNet normalization\n",
    "        if config.USE_PER_IMAGE_ZSCORE:\n",
    "            # Per-image z-score normalization\n",
    "            # Calculate mean and std per channel\n",
    "            # img is (C, H, W), compute statistics across spatial dimensions\n",
    "            mean_per_channel = img.view(3, -1).mean(dim=1, keepdim=True)  # (3, 1)\n",
    "            std_per_channel = img.view(3, -1).std(dim=1, keepdim=True)  # (3, 1)\n",
    "            \n",
    "            # Reshape for broadcasting: (3, 1, 1)\n",
    "            mean_per_channel = mean_per_channel.view(3, 1, 1)\n",
    "            std_per_channel = std_per_channel.view(3, 1, 1)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            std_per_channel = torch.clamp(std_per_channel, min=1e-6)\n",
    "            \n",
    "            # Z-score normalization: (img - mean) / std\n",
    "            img = (img - mean_per_channel) / std_per_channel\n",
    "            \n",
    "            # Clamp to [-3, 3] after z-normalization to prevent extreme values\n",
    "            img = torch.clamp(img, min=-3.0, max=3.0)\n",
    "        else:\n",
    "            # ImageNet normalization (standard normalization for pretrained models)\n",
    "            # Mean and std for ImageNet normalization\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            img = (img - mean) / std\n",
    "            \n",
    "            # Clip to [-3, 3] after normalization to prevent extreme values\n",
    "            img = torch.clamp(img, min=-3.0, max=3.0)\n",
    "        \n",
    "        # Classification label: 1.0 if mask is empty (forged), 0.0 if mask has pixels (original)\n",
    "        # Note: Forgeries have empty masks (all zeros), originals have non-empty masks\n",
    "        has_forgery = mask.sum() == 0  # Empty mask = forged\n",
    "        class_label = torch.tensor(1.0 if has_forgery else 0.0, dtype=torch.float32)\n",
    "        \n",
    "        return img, mask, class_label, weight_map\n",
    "\n",
    "# Create augmentation transforms\n",
    "if config.USE_AUGMENTATION:\n",
    "    # Joint spatial transforms (applied to both image and mask)\n",
    "    # Only apply if individual switches are enabled\n",
    "    train_joint_transform = JointTransform(\n",
    "        rotation=config.AUG_ROTATION if config.AUG_ENABLE_ROTATION else 0,\n",
    "        hflip_prob=config.AUG_HFLIP if config.AUG_ENABLE_HFLIP else 0.0,\n",
    "        vflip_prob=config.AUG_VFLIP if config.AUG_ENABLE_VFLIP else 0.0,\n",
    "        enable_rotation=config.AUG_ENABLE_ROTATION,\n",
    "        enable_hflip=config.AUG_ENABLE_HFLIP,\n",
    "        enable_vflip=config.AUG_ENABLE_VFLIP\n",
    "    )\n",
    "    \n",
    "    # Image-only color transforms (only applied to image)\n",
    "    # Only apply if color augmentation is enabled\n",
    "    if config.AUG_ENABLE_COLOR:\n",
    "        train_image_transform = transforms.Compose([\n",
    "            transforms.ColorJitter(\n",
    "                brightness=config.AUG_BRIGHTNESS,\n",
    "                contrast=config.AUG_CONTRAST,\n",
    "                saturation=config.AUG_SATURATION,\n",
    "                hue=0.0  # Keep hue unchanged for forgery detection\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        train_image_transform = None\n",
    "    \n",
    "    # Validation: no augmentations\n",
    "    val_joint_transform = None\n",
    "    val_image_transform = None\n",
    "else:\n",
    "    # Master switch disabled - no augmentations\n",
    "    train_joint_transform = None\n",
    "    train_image_transform = None\n",
    "    val_joint_transform = None\n",
    "    val_image_transform = None\n",
    "\n",
    "# Print augmentation status\n",
    "print(f\"\\nAugmentation Configuration:\")\n",
    "print(f\"  Master Switch: {'ON' if config.USE_AUGMENTATION else 'OFF'}\")\n",
    "if config.USE_AUGMENTATION:\n",
    "    print(f\"  Spatial Augmentations:\")\n",
    "    print(f\"    Rotation: {'ON' if config.AUG_ENABLE_ROTATION else 'OFF'} (angle: ±{config.AUG_ROTATION}°)\")\n",
    "    print(f\"    Horizontal Flip: {'ON' if config.AUG_ENABLE_HFLIP else 'OFF'} (prob: {config.AUG_HFLIP})\")\n",
    "    print(f\"    Vertical Flip: {'ON' if config.AUG_ENABLE_VFLIP else 'OFF'} (prob: {config.AUG_VFLIP})\")\n",
    "    print(f\"  Color Augmentations: {'ON' if config.AUG_ENABLE_COLOR else 'OFF'}\")\n",
    "    if config.AUG_ENABLE_COLOR:\n",
    "        print(f\"    Brightness: ±{config.AUG_BRIGHTNESS}\")\n",
    "        print(f\"    Contrast: ±{config.AUG_CONTRAST}\")\n",
    "        print(f\"    Saturation: ±{config.AUG_SATURATION}\")\n",
    "\n",
    "\n",
    "# Print normalization configuration\n",
    "print(f\"\\nNormalization Configuration:\")\n",
    "if config.USE_PER_IMAGE_MINMAX:\n",
    "    print(f\"  Per-Image Min-Max: ON\")\n",
    "    print(f\"    Percentile Clipping: {'ON' if config.PER_IMAGE_PERCENTILE_CLIP else 'OFF'}\")\n",
    "    if config.PER_IMAGE_PERCENTILE_CLIP:\n",
    "        print(f\"    Percentiles: {config.PER_IMAGE_LOWER_PERCENTILE}%-{config.PER_IMAGE_UPPER_PERCENTILE}%\")\n",
    "else:\n",
    "    print(f\"  Per-Image Min-Max: OFF\")\n",
    "if config.USE_PER_IMAGE_ZSCORE:\n",
    "    print(f\"  Per-Image Z-Score: ON\")\n",
    "else:\n",
    "    print(f\"  Per-Image Z-Score: OFF\")\n",
    "if config.USE_PER_IMAGE_ZSCORE:\n",
    "    print(f\"  ImageNet Normalization: OFF (replaced by per-image z-score normalization)\")\n",
    "else:\n",
    "    print(f\"  ImageNet Normalization: ON\")\n",
    "\n",
    "# Create synthetic copy-move forgery augmentation (only for training)\n",
    "# Use default values if config doesn't have EXTRA_FORGERIES parameters\n",
    "try:\n",
    "    train_synthetic_forgery = SyntheticCopyMoveForgery(\n",
    "        prob=getattr(config, 'EXTRA_FORGERIES_PROB', 0.75),\n",
    "        min_regions=getattr(config, 'EXTRA_FORGERIES_MIN_REGIONS', 3),\n",
    "        max_regions=getattr(config, 'EXTRA_FORGERIES_MAX_REGIONS', 5),\n",
    "        min_copies=getattr(config, 'EXTRA_FORGERIES_MIN_COPIES', 1),\n",
    "        max_copies=getattr(config, 'EXTRA_FORGERIES_MAX_COPIES', 1),\n",
    "        min_width=getattr(config, 'EXTRA_FORGERIES_MIN_WIDTH', 20),\n",
    "        max_width=getattr(config, 'EXTRA_FORGERIES_MAX_WIDTH', 80),\n",
    "        min_aspect=getattr(config, 'EXTRA_FORGERIES_MIN_ASPECT', 0.5),\n",
    "        max_aspect=getattr(config, 'EXTRA_FORGERIES_MAX_ASPECT', 2.0),\n",
    "        min_scale=getattr(config, 'EXTRA_FORGERIES_MIN_SCALE', 0.8),\n",
    "        max_scale=getattr(config, 'EXTRA_FORGERIES_MAX_SCALE', 1.2),\n",
    "        origin_weight=getattr(config, 'EXTRA_FORGERIES_ORIGIN_WEIGHT', 3.0),\n",
    "        probe_weight=getattr(config, 'EXTRA_FORGERIES_PROBE_WEIGHT', 1.0),\n",
    "        blur_border=getattr(config, 'EXTRA_FORGERIES_BLUR_BORDER', True),\n",
    "        origin_rotation=getattr(config, 'EXTRA_FORGERIES_ORIGIN_ROTATION', True),\n",
    "        origin_rotation_range=getattr(config, 'EXTRA_FORGERIES_ORIGIN_ROTATION_RANGE', 90),\n",
    "        probe_rotation=getattr(config, 'EXTRA_FORGERIES_PROBE_ROTATION', True),\n",
    "        probe_rotation_range=getattr(config, 'EXTRA_FORGERIES_PROBE_ROTATION_RANGE', 90)\n",
    "    )\n",
    "    print(f\"\\nSynthetic Copy-Move Forgery Augmentation:\")\n",
    "    print(f\"  Enabled for training: YES\")\n",
    "    print(f\"  Probability: {getattr(config, 'EXTRA_FORGERIES_PROB', 0.75)}\")\n",
    "    print(f\"  Regions: {getattr(config, 'EXTRA_FORGERIES_MIN_REGIONS', 3)}-{getattr(config, 'EXTRA_FORGERIES_MAX_REGIONS', 5)}\")\n",
    "    print(f\"  Copies per region: {getattr(config, 'EXTRA_FORGERIES_MIN_COPIES', 1)}-{getattr(config, 'EXTRA_FORGERIES_MAX_COPIES', 1)}\")\n",
    "    print(f\"  Origin weight: {getattr(config, 'EXTRA_FORGERIES_ORIGIN_WEIGHT', 3.0)}x | Probe weight: {getattr(config, 'EXTRA_FORGERIES_PROBE_WEIGHT', 1.0)}x\")\n",
    "except Exception as e:\n",
    "    train_synthetic_forgery = None\n",
    "    print(f\"\\nSynthetic Copy-Move Forgery Augmentation: Disabled (error: {e})\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET INITIALIZATION\n",
    "# ============================================================================\n",
    "# Training dataset with augmentations (loads from train/ subfolders)\n",
    "train_dataset = UnifiedDataset(\n",
    "    dataset_root=config.DATASET_ROOT,\n",
    "    img_size=config.IMG_SIZE,\n",
    "    transform=train_image_transform,\n",
    "    joint_transform=train_joint_transform,\n",
    "    mask_blur_radius=config.MASK_GAUSSIAN_BLUR_RADIUS,\n",
    "    split='train',  # Load from train/ subfolders\n",
    "    datasets_to_load=config.DATASETS_TO_LOAD,\n",
    "    datasets_to_skip=config.DATASETS_TO_SKIP,\n",
    "    filter_by_mask_area=config.FILTER_BY_MASK_AREA,\n",
    "    min_mask_area_percent=config.MIN_MASK_AREA_PERCENT,\n",
    "    max_mask_area_percent=config.MAX_MASK_AREA_PERCENT,\n",
    "    synthetic_forgery=train_synthetic_forgery\n",
    ")\n",
    "\n",
    "# Validation dataset without augmentations (no synthetic forgeries, loads from test/ subfolders)\n",
    "val_dataset = UnifiedDataset(\n",
    "    dataset_root=config.DATASET_ROOT,\n",
    "    img_size=config.IMG_SIZE,\n",
    "    transform=val_image_transform,  # None (no color augmentation)\n",
    "    joint_transform=val_joint_transform,  # None (no spatial augmentation)\n",
    "    mask_blur_radius=0.0,  # No mask blur for validation\n",
    "    split='test',  # Load from test/ subfolders\n",
    "    datasets_to_load=config.DATASETS_TO_LOAD,\n",
    "    datasets_to_skip=config.DATASETS_TO_SKIP,\n",
    "    filter_by_mask_area=config.FILTER_BY_MASK_AREA,\n",
    "    min_mask_area_percent=config.MIN_MASK_AREA_PERCENT,\n",
    "    max_mask_area_percent=config.MAX_MASK_AREA_PERCENT,\n",
    "    synthetic_forgery=None  # No synthetic forgeries for validation\n",
    ")\n",
    "\n",
    "# Create full dataset for statistics (loads from both train and test)\n",
    "full_dataset = UnifiedDataset(\n",
    "    dataset_root=config.DATASET_ROOT,\n",
    "    img_size=config.IMG_SIZE,\n",
    "    transform=None,\n",
    "    joint_transform=None,\n",
    "    mask_blur_radius=0.0,\n",
    "    split='all',  # Load from both train/ and test/ subfolders\n",
    "    datasets_to_load=config.DATASETS_TO_LOAD,\n",
    "    datasets_to_skip=config.DATASETS_TO_SKIP,\n",
    "    filter_by_mask_area=config.FILTER_BY_MASK_AREA,\n",
    "    min_mask_area_percent=config.MIN_MASK_AREA_PERCENT,\n",
    "    max_mask_area_percent=config.MAX_MASK_AREA_PERCENT\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal images (train + test): {len(full_dataset)}\")\n",
    "# Efficiently count forged vs original by checking mask files directly (without loading images)\n",
    "# Note: Forgeries have empty masks (all zeros), originals have non-empty masks\n",
    "forged_count = 0\n",
    "for mask_path in full_dataset.mask_paths:\n",
    "    mask = np.load(mask_path)\n",
    "    # Check if mask is empty (all zeros) - empty mask = forged image\n",
    "    if not np.any(mask > 0):\n",
    "        forged_count += 1\n",
    "original_count = len(full_dataset) - forged_count\n",
    "print(f\"  Images with forgeries: {forged_count}\")\n",
    "print(f\"  Images without forgeries: {original_count}\")\n",
    "\n",
    "print(f\"\\nTrain dataset: {len(train_dataset)} images\")\n",
    "print(f\"Val dataset: {len(val_dataset)} images\")\n",
    "\n",
    "# Test one sample\n",
    "sample_img, sample_mask, sample_class, sample_weight = train_dataset[0]\n",
    "print(f\"\\nSample shape - Image: {sample_img.shape}, Mask: {sample_mask.shape}, Class: {sample_class}, Weight: {sample_weight.shape}\")\n",
    "print(f\"Image dtype: {sample_img.dtype}, Mask dtype: {sample_mask.dtype}, Class dtype: {sample_class.dtype}, Weight dtype: {sample_weight.dtype}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET VISUALIZATION\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize first 5 samples from the dataset\n",
    "num_samples = 5\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "# Simply take first 5 samples from the top\n",
    "selected_indices = list(range(min(num_samples, len(full_dataset))))\n",
    "\n",
    "for row_idx, dataset_idx in enumerate(selected_indices):\n",
    "    img, mask, class_label, weight_map = full_dataset[dataset_idx]\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    img_np = img.permute(1, 2, 0).cpu().numpy()  # (C, H, W) -> (H, W, C)\n",
    "    mask_np = mask.squeeze().cpu().numpy()  # (1, H, W) -> (H, W)\n",
    "    \n",
    "    # Denormalize image\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_np = img_np * std + mean\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    # Determine image type based on mask content\n",
    "    # Note: Forgeries have empty masks (all zeros), originals have non-empty masks\n",
    "    is_forged = not mask_np.any()  # Empty mask = forged\n",
    "    image_type = \"Forged\" if is_forged else \"Original/Authentic\"\n",
    "    mask_pixels = mask_np.sum()  # Only compute sum for display\n",
    "    \n",
    "    # Original image\n",
    "    axes[row_idx, 0].imshow(img_np)\n",
    "    axes[row_idx, 0].set_title(f'{image_type} Image\\n({mask_pixels:.0f} mask pixels)')\n",
    "    axes[row_idx, 0].axis('off')\n",
    "    \n",
    "    # Ground truth mask\n",
    "    axes[row_idx, 1].imshow(mask_np, cmap='gray')\n",
    "    axes[row_idx, 1].set_title(f'Ground Truth Mask\\n({mask_pixels:.0f} pixels)')\n",
    "    axes[row_idx, 1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = img_np.copy()\n",
    "    mask_colored = np.zeros_like(overlay)\n",
    "    mask_colored[:, :, 0] = mask_np  # Red channel\n",
    "    overlay = np.clip(overlay * 0.7 + mask_colored * 0.3, 0, 1)\n",
    "    axes[row_idx, 2].imshow(overlay)\n",
    "    axes[row_idx, 2].set_title(f'{image_type} + Mask Overlay')\n",
    "    axes[row_idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_samples_visualization.jpg', dpi=150, bbox_inches='tight', format='jpg')\n",
    "print(\"Dataset visualization saved to: dataset_samples_visualization.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET SPLIT AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SPLIT AND SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use train_dataset for training (with augmentations and synthetic forgeries)\n",
    "# Use val_dataset for validation (clean, no augmentations, from test/ folders)\n",
    "train_dataset_split = train_dataset\n",
    "val_dataset_split = val_dataset\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Train: {len(train_dataset_split)} images (from train/ folders with augmentations)\")\n",
    "print(f\"  Val: {len(val_dataset_split)} images (from test/ folders, no augmentations)\")\n",
    "print(f\"  Note: Train uses augmented data with synthetic forgeries, Val uses clean test data\")\n",
    "\n",
    "# Create dataloaders (will be updated by curriculum learning if enabled)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_split,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_split,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train loader: {len(train_loader)} batches\")\n",
    "print(f\"  Validation loader: {len(val_loader)} batches\")\n",
    "\n",
    "# Initialize training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_iou': [],\n",
    "    'train_f1': [],\n",
    "    'val_loss': [],\n",
    "    'val_iou': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "patience = config.PATIENCE\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Setup complete! Ready for training.\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5981597,
     "status": "ok",
     "timestamp": 1751822053708,
     "user": {
      "displayName": "JAYESH SHARMA",
      "userId": "10181368842352240411"
     },
     "user_tz": -330
    },
    "id": "vmaKQh1WaFk0",
    "outputId": "efcc0038-ab88-4787-8a03-370e6dcf3c38"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setup AMP (Automatic Mixed Precision)\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16\n",
    "        print(f\"✓ GPU supports bfloat16 - Using AMP with bfloat16\")\n",
    "    else:\n",
    "        amp_dtype = torch.float16\n",
    "        print(f\"⚠ GPU does not support bfloat16 - Falling back to float16\")\n",
    "else:\n",
    "    amp_dtype = None\n",
    "    print(f\"⚠ CUDA not available - AMP disabled\")\n",
    "\n",
    "# Create GradScaler for AMP\n",
    "scaler = GradScaler('cuda') if device.type == 'cuda' else None\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.VIZ_DIR, exist_ok=True)\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nOutput directories created:\")\n",
    "print(f\"  Main output: {config.OUTPUT_DIR}/\")\n",
    "print(f\"  Visualizations: {config.VIZ_DIR}/\")\n",
    "print(f\"  Checkpoints: {config.CHECKPOINT_DIR}/\")\n",
    "\n",
    "# Use loss functions, metrics, and visualization functions from helper cells (Cells 3 and 4)\n",
    "# Loss functions: CombinedLossWithDeepSupervision (Cell 3)\n",
    "# Metrics: calculate_iou, calculate_f1_score, calculate_metrics (Cell 3)\n",
    "# Visualization functions: save_visualization (Cell 4)\n",
    "    \n",
    "# Create loss function and optimizer\n",
    "criterion = CombinedLossWithDeepSupervision(\n",
    "    focal_weight=config.FOCAL_WEIGHT,\n",
    "    dice_weight=config.DICE_WEIGHT,\n",
    "    dice_smooth=config.DICE_SMOOTH,\n",
    "    focal_alpha=config.FOCAL_ALPHA,\n",
    "    focal_gamma=config.FOCAL_GAMMA,\n",
    "    deep_supervision_weights=None,  # Deep supervision disabled\n",
    "    use_dice_loss=config.USE_DICE_LOSS\n",
    ")\n",
    "\n",
    "if config.OPTIMIZER.lower() == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "elif config.OPTIMIZER.lower() == 'adamw':\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "elif config.OPTIMIZER.lower() == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY, momentum=0.9)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown optimizer: {config.OPTIMIZER}. Use 'adam', 'adamw', or 'sgd'.\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total epochs: {config.NUM_EPOCHS}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_iou_sum = 0.0\n",
    "    train_f1_sum = 0.0\n",
    "    train_batches = 0\n",
    "    train_images_batch = None\n",
    "    train_masks_batch = None\n",
    "    train_outputs_batch = None\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.NUM_EPOCHS} [Train]')\n",
    "    \n",
    "    # Get total number of batches for last batch capture\n",
    "    total_train_batches = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (images, masks, class_labels, weight_maps) in enumerate(train_pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        weight_maps = weight_maps.to(device)\n",
    "        \n",
    "        # Forward pass with AMP (Automatic Mixed Precision)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use autocast context for forward pass (uses bfloat16 or float16)\n",
    "        with autocast(device_type='cuda', dtype=amp_dtype):\n",
    "            mask_outputs = model(images)\n",
    "            \n",
    "            # Calculate loss (segmentation only) with weight maps\n",
    "            loss = criterion(mask_outputs, masks, weight_maps)\n",
    "        \n",
    "        # Backward pass\n",
    "        # Use scaler for gradient scaling (important for fp16, optional for bf16)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            # Unscale gradients before clipping (required when using scaler)\n",
    "            scaler.unscale_(optimizer)\n",
    "            # Clip gradient norm to 1.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            # Clip gradient norm to 1.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Save second-to-last batch for visualization\n",
    "        if total_train_batches > 1 and batch_idx == total_train_batches - 2:\n",
    "            train_images_batch = images.detach().clone()\n",
    "            train_masks_batch = masks.detach().clone()\n",
    "            # Handle deep supervision outputs (tuple) or single output (tensor)\n",
    "            if isinstance(mask_outputs, tuple):\n",
    "                train_outputs_batch = torch.sigmoid(mask_outputs[0].detach().clone())  # Use main output\n",
    "            else:\n",
    "                train_outputs_batch = torch.sigmoid(mask_outputs.detach().clone())  # Convert logits to probs\n",
    "        \n",
    "        # Calculate metrics using helper functions\n",
    "        train_loss += loss.item()\n",
    "        iou, f1 = calculate_metrics(mask_outputs, masks, threshold=config.PREDICTION_THRESHOLD)\n",
    "        \n",
    "        train_iou_sum += iou.item()\n",
    "        train_f1_sum += f1.item()\n",
    "        \n",
    "        train_batches += 1\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'iou': f'{iou.item():.4f}',\n",
    "            'f1': f'{f1.item():.4f}',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Periodic cache clearing during batch processing (every 50 batches)\n",
    "        # Prevents memory buildup during long epochs\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_iou = train_iou_sum / train_batches if train_batches > 0 else 0.0\n",
    "    train_f1 = train_f1_sum / train_batches if train_batches > 0 else 0.0\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    if 'train_f1' not in history:\n",
    "        history['train_f1'] = []\n",
    "    history['train_f1'].append(train_f1)\n",
    "    \n",
    "    # Save training visualizations (should always be available since we save batch 0)\n",
    "    if train_images_batch is not None:\n",
    "        try:\n",
    "            save_visualization(train_images_batch, train_masks_batch, train_outputs_batch, \n",
    "                             epoch + 1, split='train')\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to save training visualizations: {e}\")\n",
    "    else:\n",
    "        print(f\"  Warning: No training batch captured for visualization in epoch {epoch + 1}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou_sum = 0.0\n",
    "    val_f1_sum = 0.0\n",
    "    val_batches = 0\n",
    "    val_images_batch = None\n",
    "    val_masks_batch = None\n",
    "    val_outputs_batch = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config.NUM_EPOCHS} [Val]')\n",
    "        \n",
    "        # Get total number of batches for last batch capture\n",
    "        total_val_batches = len(val_loader)\n",
    "        \n",
    "        for batch_idx, (images, masks, class_labels, weight_maps) in enumerate(val_pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            weight_maps = weight_maps.to(device)\n",
    "            \n",
    "            # Use autocast for validation forward pass as well\n",
    "            with autocast(device_type='cuda', dtype=amp_dtype):\n",
    "                mask_outputs = model(images)\n",
    "                loss = criterion(mask_outputs, masks, weight_maps)\n",
    "            \n",
    "            # Save second-to-last batch for visualization\n",
    "            if total_val_batches > 1 and batch_idx == total_val_batches - 2:\n",
    "                val_images_batch = images.clone()  # Already in no_grad context\n",
    "                val_masks_batch = masks.clone()\n",
    "                # Handle deep supervision outputs (tuple) or single output (tensor)\n",
    "                if isinstance(mask_outputs, tuple):\n",
    "                    val_outputs_batch = torch.sigmoid(mask_outputs[0].clone())  # Use main output\n",
    "                else:\n",
    "                    val_outputs_batch = torch.sigmoid(mask_outputs.clone())  # Convert logits to probs\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics using helper functions\n",
    "            iou, f1 = calculate_metrics(mask_outputs, masks, threshold=config.PREDICTION_THRESHOLD)\n",
    "            \n",
    "            val_iou_sum += iou.item()\n",
    "            val_f1_sum += f1.item()\n",
    "            \n",
    "            val_batches += 1\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'iou': f'{iou.item():.4f}',\n",
    "                'f1': f'{f1.item():.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_iou = val_iou_sum / val_batches if val_batches > 0 else 0.0\n",
    "    val_f1 = val_f1_sum / val_batches if val_batches > 0 else 0.0\n",
    "    \n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    if 'val_f1' not in history:\n",
    "        history['val_f1'] = []\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Save validation visualizations (should always be available since we save batch 0)\n",
    "    if val_images_batch is not None:\n",
    "        try:\n",
    "            save_visualization(val_images_batch, val_masks_batch, val_outputs_batch, \n",
    "                             epoch + 1, split='val')\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to save validation visualizations: {e}\")\n",
    "    else:\n",
    "        print(f\"  Warning: No validation batch captured for visualization in epoch {epoch + 1}\")\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1}/{config.NUM_EPOCHS}:')\n",
    "    print(f'  Train - Loss: {avg_train_loss:.4f}, IoU: {train_iou:.4f}, F1: {train_f1:.4f}')\n",
    "    print(f'  Val   - Loss: {avg_val_loss:.4f}, IoU: {val_iou:.4f}, F1: {val_f1:.4f}')\n",
    "    print(f'  Visualizations saved to {config.VIZ_DIR}/')\n",
    "    \n",
    "    # Model checkpointing - always save last checkpoint (overwrite)\n",
    "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, config.MODEL_SAVE_NAME)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f'  ✓ Checkpoint saved: {checkpoint_path} (Val Loss: {avg_val_loss:.4f})')\n",
    "    \n",
    "    # Track best loss for early stopping (but always save last checkpoint)\n",
    "    if avg_val_loss < best_val_loss - config.MIN_DELTA:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f'  ✓ New best validation loss: {best_val_loss:.4f}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  No improvement. Patience: {patience_counter}/{patience}')\n",
    "    \n",
    "    # Plot and save training history graph (updated each epoch)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # IoU subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_iou'], label='Train IoU', marker='o')\n",
    "    plt.plot(history['val_iou'], label='Val IoU', marker='o')\n",
    "    plt.legend()\n",
    "    plt.title('IoU Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot (overwrites each epoch)\n",
    "    history_plot_path = os.path.join(config.OUTPUT_DIR, 'training_history.jpg')\n",
    "    plt.savefig(history_plot_path, dpi=150, bbox_inches='tight', format='jpg')\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    print(f'  ✓ Training history graph saved: {history_plot_path}')\n",
    "    \n",
    "    # Clear cache to prevent performance degradation over time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache\n",
    "    gc.collect()  # Force Python garbage collection\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f'\\nBest model loaded with validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# IoU\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_iou'], label='Train IoU')\n",
    "plt.plot(history['val_iou'], label='Val IoU')\n",
    "plt.legend()\n",
    "plt.title('IoU Evolution')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE ON TEST DATASET\n",
    "# ============================================================================\n",
    "# Load checkpoint from config and evaluate on combined_dataset/science-fraud_copymove/test\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE ON TEST DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration for inference\n",
    "# Use checkpoint from config\n",
    "INFERENCE_CHECKPOINT = os.path.join(config.CHECKPOINT_DIR, config.MODEL_SAVE_NAME)\n",
    "TEST_DATASET_ROOT = 'combined_dataset'  # Dataset root\n",
    "TEST_DATASET_NAME = 'science-fraud_copymove'  # Specific dataset\n",
    "TEST_SPLIT = 'test'  # Test split\n",
    "PREDICTION_THRESHOLD = 0.5  # Threshold for binary predictions\n",
    "BATCH_SIZE = 8  # Batch size for inference\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if not os.path.exists(INFERENCE_CHECKPOINT):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {INFERENCE_CHECKPOINT}\")\n",
    "\n",
    "print(f\"\\n✓ Checkpoint found: {INFERENCE_CHECKPOINT}\")\n",
    "\n",
    "# Create model (same architecture as training)\n",
    "print(\"\\nCreating model...\")\n",
    "inference_model = MaxViT_CBAM_UNet(\n",
    "    in_channels=config.IN_CHANNELS,\n",
    "    out_channels=config.OUT_CHANNELS,\n",
    "    r=config.CBAM_REDUCTION,\n",
    "    skip_pretrained=True,  # We're loading from checkpoint\n",
    "    use_deep_supervision=config.USE_DEEP_SUPERVISION,\n",
    "    use_gradient_checkpointing=False,  # Disable for inference\n",
    "    use_atrous_pyramid_bottleneck=config.USE_ATROUS_PYRAMID_BOTTLENECK,\n",
    "    use_identity_bottleneck=config.USE_IDENTITY_BOTTLENECK\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint: {INFERENCE_CHECKPOINT}\")\n",
    "checkpoint = torch.load(INFERENCE_CHECKPOINT, map_location=device)\n",
    "\n",
    "# Handle different checkpoint formats\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    print(f\"✓ Loaded complete checkpoint (epoch: {checkpoint.get('epoch', 'N/A')})\")\n",
    "else:\n",
    "    model_state_dict = checkpoint\n",
    "    print(\"✓ Loaded state dict checkpoint\")\n",
    "\n",
    "# Load weights\n",
    "inference_model.load_state_dict(model_state_dict, strict=False)\n",
    "inference_model.eval()\n",
    "print(f\"✓ Model loaded successfully on {device}\")\n",
    "\n",
    "# Create test dataset (no augmentations, no synthetic forgeries)\n",
    "print(f\"\\nLoading test dataset: {TEST_DATASET_ROOT}/{TEST_DATASET_NAME}/{TEST_SPLIT}\")\n",
    "test_dataset = UnifiedDataset(\n",
    "    dataset_root=TEST_DATASET_ROOT,\n",
    "    img_size=config.IMG_SIZE,\n",
    "    transform=None,  # No image transforms for inference\n",
    "    joint_transform=None,  # No spatial transforms for inference\n",
    "    mask_blur_radius=0.0,  # No blur for inference\n",
    "    split=TEST_SPLIT,\n",
    "    datasets_to_load=[TEST_DATASET_NAME],  # Only load the specified dataset\n",
    "    datasets_to_skip=[],\n",
    "    filter_by_mask_area=False,  # Don't filter for inference\n",
    "    synthetic_forgery=None  # No synthetic forgeries for inference\n",
    ")\n",
    "\n",
    "print(f\"✓ Test dataset loaded: {len(test_dataset)} images\")\n",
    "\n",
    "# Create data loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nRunning inference...\")\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "iou_per_image = []\n",
    "f1_per_image = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, masks, _, _) in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = inference_model(images)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Threshold to get binary predictions\n",
    "        predictions = (probs > PREDICTION_THRESHOLD).float()\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_targets.append(masks.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "        \n",
    "        # Calculate per-image metrics\n",
    "        batch_size = images.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            pred_i = predictions[i:i+1]\n",
    "            target_i = masks[i:i+1]\n",
    "            prob_i = probs[i:i+1]\n",
    "            \n",
    "            # Calculate IoU for this image\n",
    "            intersection = (pred_i * target_i).sum()\n",
    "            union = pred_i.sum() + target_i.sum() - intersection\n",
    "            iou = (intersection / (union + 1e-6)).item() if union > 0 else 0.0\n",
    "            \n",
    "            # Calculate F1 for this image\n",
    "            pred_flat = pred_i.view(-1)\n",
    "            target_flat = target_i.view(-1)\n",
    "            tp = (pred_flat * target_flat).sum().item()\n",
    "            fp = (pred_flat * (1 - target_flat)).sum().item()\n",
    "            fn = ((1 - pred_flat) * target_flat).sum().item()\n",
    "            \n",
    "            precision = (tp + 1e-6) / (tp + fp + 1e-6)\n",
    "            recall = (tp + 1e-6) / (tp + fn + 1e-6)\n",
    "            f1 = (2 * precision * recall) / (precision + recall + 1e-6)\n",
    "            \n",
    "            iou_per_image.append(iou)\n",
    "            f1_per_image.append(f1)\n",
    "\n",
    "# Concatenate all results\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "print(f\"\\n✓ Inference complete: {len(all_predictions)} images processed\")\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULATE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CALCULATING METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Forgery IoU (mean IoU across all images)\n",
    "forgery_iou = np.mean(iou_per_image)\n",
    "print(f\"\\n1. Forgery IoU: {forgery_iou:.4f}\")\n",
    "\n",
    "# 2. Pixel F1 (overall pixel-wise F1 score)\n",
    "pred_flat = all_predictions.view(-1).numpy()\n",
    "target_flat = all_targets.view(-1).numpy()\n",
    "tp = np.sum((pred_flat == 1) & (target_flat == 1))\n",
    "fp = np.sum((pred_flat == 1) & (target_flat == 0))\n",
    "fn = np.sum((pred_flat == 0) & (target_flat == 1))\n",
    "tn = np.sum((pred_flat == 0) & (target_flat == 0))\n",
    "\n",
    "pixel_precision = (tp + 1e-6) / (tp + fp + 1e-6)\n",
    "pixel_recall = (tp + 1e-6) / (tp + fn + 1e-6)\n",
    "pixel_f1 = (2 * pixel_precision * pixel_recall) / (pixel_precision + pixel_recall + 1e-6)\n",
    "print(f\"2. Pixel F1: {pixel_f1:.4f}\")\n",
    "print(f\"   Pixel Precision: {pixel_precision:.4f}\")\n",
    "print(f\"   Pixel Recall: {pixel_recall:.4f}\")\n",
    "\n",
    "# 3. Macro Image F1 (average F1 per image, then mean)\n",
    "macro_image_f1 = np.mean(f1_per_image)\n",
    "print(f\"3. Macro Image F1: {macro_image_f1:.4f}\")\n",
    "\n",
    "# 4. Confusion Matrix (pixel-level)\n",
    "print(f\"\\n4. Confusion Matrix (Pixel-level):\")\n",
    "print(f\"   True Positives (TP): {tp:,}\")\n",
    "print(f\"   False Positives (FP): {fp:,}\")\n",
    "print(f\"   False Negatives (FN): {fn:,}\")\n",
    "print(f\"   True Negatives (TN): {tn:,}\")\n",
    "\n",
    "# Create confusion matrix array\n",
    "cm = np.array([[tn, fp],\n",
    "               [fn, tp]])\n",
    "\n",
    "print(f\"\\n   Confusion Matrix:\")\n",
    "print(f\"   {'':>15} {'Predicted 0':>15} {'Predicted 1':>15}\")\n",
    "print(f\"   {'Actual 0':>15} {tn:>15,} {fp:>15,}\")\n",
    "print(f\"   {'Actual 1':>15} {fn:>15,} {tp:>15,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE CONFUSION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "ax1 = axes[0]\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Background', 'Forgery'])\n",
    "cm_display.plot(ax=ax1, cmap='Blues', values_format='d')\n",
    "ax1.set_title('Confusion Matrix (Pixel-level)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "ax1.set_ylabel('Actual', fontsize=12)\n",
    "\n",
    "# Metrics summary\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "metrics_text = f\"\"\"\n",
    "METRICS SUMMARY\n",
    "\n",
    "Forgery IoU:     {forgery_iou:.4f}\n",
    "Pixel F1:        {pixel_f1:.4f}\n",
    "Macro Image F1:  {macro_image_f1:.4f}\n",
    "\n",
    "Pixel-level Statistics:\n",
    "  Precision:     {pixel_precision:.4f}\n",
    "  Recall:        {pixel_recall:.4f}\n",
    "  \n",
    "Confusion Matrix:\n",
    "  TP: {tp:,}\n",
    "  FP: {fp:,}\n",
    "  FN: {fn:,}\n",
    "  TN: {tn:,}\n",
    "  \n",
    "Total Pixels:    {len(pred_flat):,}\n",
    "Forgery Pixels:  {np.sum(target_flat):,} ({100*np.sum(target_flat)/len(target_flat):.2f}%)\n",
    "\"\"\"\n",
    "ax2.text(0.1, 0.5, metrics_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('inference_results.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n✓ Results saved to: inference_results.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {TEST_DATASET_ROOT}/{TEST_DATASET_NAME}/{TEST_SPLIT}\")\n",
    "print(f\"Checkpoint: {INFERENCE_CHECKPOINT}\")\n",
    "print(f\"Total Images: {len(test_dataset)}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Forgery IoU:     {forgery_iou:.4f}\")\n",
    "print(f\"  Pixel F1:        {pixel_f1:.4f}\")\n",
    "print(f\"  Macro Image F1:  {macro_image_f1:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE INFERENCE ON ALL 3 TEST DATASETS\n",
      "================================================================================\n",
      "\n",
      "✓ Loading checkpoint: outputs_unet/checkpoints/MaxVit-square-Unet-fraud-best.pth\n",
      "Creating MaxViT model without pretrained weights (skip_pretrained=True)...\n",
      "  Initializing model with random weights...\n",
      "✓ Model loaded and set to eval mode\n",
      "\n",
      "================================================================================\n",
      "PROCESSING DATASET: casia_copymove\n",
      "================================================================================\n",
      "Found 115 image-mask pairs from 1 subfolders (split: test)\n",
      "  Loaded datasets: casia_copymove\n",
      "  (Filtered to: ['casia_copymove'])\n",
      "✓ Found 115 test images\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference casia_copymove: 100%|██████████| 15/15 [00:05<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE REVIEW: CASIA_COPYMOVE\n",
      "================================================================================\n",
      "\n",
      "Dataset: casia_copymove/test\n",
      "Total Images: 115\n",
      "\n",
      "📊 OVERALL METRICS:\n",
      "  Forgery IoU:        0.1420\n",
      "  Pixel-wise F1:      0.2487\n",
      "  Macro Image F1:     0.2331\n",
      "  Mean IoU per Image: 0.1245\n",
      "\n",
      "📈 PIXEL-LEVEL STATISTICS:\n",
      "  Accuracy:           0.9285\n",
      "  Precision:          0.3559\n",
      "  Recall:             0.1912\n",
      "\n",
      "🔢 CONFUSION MATRIX (Pixel-level):\n",
      "                      Predicted 0     Predicted 1\n",
      "         Actual 0      27,632,929         646,177\n",
      "         Actual 1       1,510,427         357,027\n",
      "\n",
      "  Total Pixels:       30,146,560\n",
      "  Forgery Pixels:     1,867,454 (6.19%)\n",
      "\n",
      "📊 IMAGE-LEVEL STATISTICS:\n",
      "  Accuracy:           0.6000\n",
      "  Precision:          0.5980\n",
      "  Recall:             0.9242\n",
      "  F1 Score:           0.7262\n",
      "\n",
      "🔢 CONFUSION MATRIX (Image-level):\n",
      "                      Predicted 0     Predicted 1\n",
      "         Actual 0               8              41\n",
      "         Actual 1               5              61\n",
      "================================================================================\n",
      "\n",
      "💾 Image-level confusion matrix saved to: outputs_unet/test_inference_strips/casia_copymove/casia_copymove_image_level_confusion_matrix.png\n",
      "💾 Pixel-level confusion matrix saved to: outputs_unet/test_inference_strips/casia_copymove/casia_copymove_pixel_level_confusion_matrix.png\n",
      "\n",
      "💾 Saving first 10 image strips for casia_copymove...\n",
      "  ✓ Saved 10 image strips to outputs_unet/test_inference_strips/casia_copymove\n",
      "\n",
      "================================================================================\n",
      "PROCESSING DATASET: defacto_copymove\n",
      "================================================================================\n",
      "Found 264 image-mask pairs from 1 subfolders (split: test)\n",
      "  Loaded datasets: defacto_copymove\n",
      "  (Filtered to: ['defacto_copymove'])\n",
      "✓ Found 264 test images\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference defacto_copymove: 100%|██████████| 33/33 [00:10<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE REVIEW: DEFACTO_COPYMOVE\n",
      "================================================================================\n",
      "\n",
      "Dataset: defacto_copymove/test\n",
      "Total Images: 264\n",
      "\n",
      "📊 OVERALL METRICS:\n",
      "  Forgery IoU:        0.6961\n",
      "  Pixel-wise F1:      0.8208\n",
      "  Macro Image F1:     0.7818\n",
      "  Mean IoU per Image: 0.6907\n",
      "\n",
      "📈 PIXEL-LEVEL STATISTICS:\n",
      "  Accuracy:           0.9713\n",
      "  Precision:          0.9011\n",
      "  Recall:             0.7536\n",
      "\n",
      "🔢 CONFUSION MATRIX (Pixel-level):\n",
      "                      Predicted 0     Predicted 1\n",
      "         Actual 0      62,673,175         498,892\n",
      "         Actual 1       1,486,682       4,547,267\n",
      "\n",
      "  Total Pixels:       69,206,016\n",
      "  Forgery Pixels:     6,033,949 (8.72%)\n",
      "\n",
      "📊 IMAGE-LEVEL STATISTICS:\n",
      "  Accuracy:           0.9848\n",
      "  Precision:          1.0000\n",
      "  Recall:             0.9848\n",
      "  F1 Score:           0.9924\n",
      "\n",
      "🔢 CONFUSION MATRIX (Image-level):\n",
      "                      Predicted 0     Predicted 1\n",
      "         Actual 0               0               0\n",
      "         Actual 1               4             260\n",
      "================================================================================\n",
      "\n",
      "💾 Image-level confusion matrix saved to: outputs_unet/test_inference_strips/defacto_copymove/defacto_copymove_image_level_confusion_matrix.png\n",
      "💾 Pixel-level confusion matrix saved to: outputs_unet/test_inference_strips/defacto_copymove/defacto_copymove_pixel_level_confusion_matrix.png\n",
      "\n",
      "💾 Saving first 10 image strips for defacto_copymove...\n",
      "  ✓ Saved 10 image strips to outputs_unet/test_inference_strips/defacto_copymove\n",
      "\n",
      "================================================================================\n",
      "PROCESSING DATASET: science-fraud_copymove\n",
      "================================================================================\n",
      "Found 213 image-mask pairs from 1 subfolders (split: test)\n",
      "  Loaded datasets: science-fraud_copymove\n",
      "  (Filtered to: ['science-fraud_copymove'])\n",
      "✓ Found 213 test images\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference science-fraud_copymove: 100%|██████████| 27/27 [00:08<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE REVIEW: SCIENCE-FRAUD_COPYMOVE\n",
      "================================================================================\n",
      "\n",
      "Dataset: science-fraud_copymove/test\n",
      "Total Images: 213\n",
      "\n",
      "📊 OVERALL METRICS:\n",
      "  Forgery IoU:        0.2557\n",
      "  Pixel-wise F1:      0.4072\n",
      "  Macro Image F1:     0.4504\n",
      "  Mean IoU per Image: 0.1264\n",
      "\n",
      "📈 PIXEL-LEVEL STATISTICS:\n",
      "  Accuracy:           0.9780\n",
      "  Precision:          0.6136\n",
      "  Recall:             0.3047\n",
      "\n",
      "🔢 CONFUSION MATRIX (Pixel-level):\n",
      "                      Predicted 0     Predicted 1\n",
      "         Actual 0      54,183,570         266,141\n",
      "         Actual 1         964,290         422,671\n",
      "\n",
      "  Total Pixels:       55,836,672\n",
      "  Forgery Pixels:     1,386,961 (2.48%)\n",
      "\n",
      "📊 IMAGE-LEVEL STATISTICS:\n",
      "  Accuracy:           0.6667\n",
      "  Precision:          0.6860\n",
      "  Recall:             0.7155\n",
      "  F1 Score:           0.7004\n",
      "\n",
      "🔢 CONFUSION MATRIX (Image-level):\n",
      "                      Predicted 0     Predicted 1\n",
      "         Actual 0              59              38\n",
      "         Actual 1              33              83\n",
      "================================================================================\n",
      "\n",
      "💾 Image-level confusion matrix saved to: outputs_unet/test_inference_strips/science-fraud_copymove/science-fraud_copymove_image_level_confusion_matrix.png\n",
      "💾 Pixel-level confusion matrix saved to: outputs_unet/test_inference_strips/science-fraud_copymove/science-fraud_copymove_pixel_level_confusion_matrix.png\n",
      "\n",
      "💾 Saving first 10 image strips for science-fraud_copymove...\n",
      "  ✓ Saved 10 image strips to outputs_unet/test_inference_strips/science-fraud_copymove\n",
      "\n",
      "================================================================================\n",
      "OVERALL SUMMARY - ALL DATASETS\n",
      "================================================================================\n",
      "\n",
      "               Dataset  Images Forgery IoU Pixel F1 Macro Image F1 Pixel Precision Pixel Recall\n",
      "        casia_copymove     115      0.1420   0.2487         0.2331          0.3559       0.1912\n",
      "      defacto_copymove     264      0.6961   0.8208         0.7818          0.9011       0.7536\n",
      "science-fraud_copymove     213      0.2557   0.4072         0.4504          0.6136       0.3047\n",
      "\n",
      "📊 AVERAGE ACROSS ALL DATASETS:\n",
      "  Average Forgery IoU:    0.3646\n",
      "  Average Pixel F1:        0.4923\n",
      "  Average Macro Image F1:  0.4884\n",
      "\n",
      "💾 Creating combined confusion matrix visualization...\n",
      "  ✓ Combined confusion matrix saved to: outputs_unet/test_inference_strips/combined_confusion_matrices.png\n",
      "\n",
      "💾 Visualizations saved to: outputs_unet/test_inference_strips\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE INFERENCE ON ALL 3 TEST DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for saving\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE INFERENCE ON ALL 3 TEST DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test datasets to process\n",
    "TEST_DATASETS = [\n",
    "    'casia_copymove',\n",
    "    'defacto_copymove', \n",
    "    'science-fraud_copymove'\n",
    "]\n",
    "\n",
    "# Load model for inference\n",
    "INFERENCE_CHECKPOINT = os.path.join(config.CHECKPOINT_DIR, config.MODEL_SAVE_NAME)\n",
    "if not os.path.exists(INFERENCE_CHECKPOINT):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {INFERENCE_CHECKPOINT}\")\n",
    "\n",
    "print(f\"\\n✓ Loading checkpoint: {INFERENCE_CHECKPOINT}\")\n",
    "\n",
    "# Create inference model\n",
    "inference_model = MaxViT_CBAM_UNet(\n",
    "    in_channels=config.IN_CHANNELS,\n",
    "    out_channels=config.OUT_CHANNELS,\n",
    "    r=config.CBAM_REDUCTION,\n",
    "    skip_pretrained=True,\n",
    "    use_gradient_checkpointing=False,\n",
    "    use_atrous_pyramid_bottleneck=config.USE_ATROUS_PYRAMID_BOTTLENECK,\n",
    "    use_identity_bottleneck=config.USE_IDENTITY_BOTTLENECK\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(INFERENCE_CHECKPOINT, map_location=device)\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "else:\n",
    "    model_state_dict = checkpoint\n",
    "\n",
    "inference_model.load_state_dict(model_state_dict, strict=False)\n",
    "inference_model.eval()\n",
    "print(\"✓ Model loaded and set to eval mode\")\n",
    "\n",
    "# Create output directory for visualizations\n",
    "VIZ_OUTPUT_DIR = Path('outputs_unet/test_inference_strips')\n",
    "VIZ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to create image strip (image + gt + pred + overlay)\n",
    "def create_image_strip(img_array, gt_mask, pred_mask, output_path, img_name, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Create a 4-panel strip: Original | GT Mask | Pred Mask | Overlay\n",
    "    \"\"\"\n",
    "    # Normalize image to [0, 1] if needed\n",
    "    if img_array.max() > 1.0:\n",
    "        img_array = img_array.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        img_array = img_array.astype(np.float32)\n",
    "    \n",
    "    # Ensure masks are binary [0, 1]\n",
    "    gt_mask = (gt_mask > 0.5).astype(np.float32)\n",
    "    pred_mask = (pred_mask > 0.5).astype(np.float32)\n",
    "    \n",
    "    # Create figure with 4 panels\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Panel 1: Original image\n",
    "    axes[0].imshow(img_array)\n",
    "    axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Panel 2: Ground truth mask\n",
    "    axes[1].imshow(gt_mask, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title(f'GT Mask\\n({int(gt_mask.sum())} px)', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Panel 3: Predicted mask\n",
    "    axes[2].imshow(pred_mask, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2].set_title(f'Pred Mask\\n({int(pred_mask.sum())} px)', fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Panel 4: Overlay (GT=red, Pred=blue)\n",
    "    overlay = img_array.copy()\n",
    "    gt_colored = np.zeros_like(overlay)\n",
    "    gt_colored[:, :, 0] = gt_mask  # Red for GT\n",
    "    pred_colored = np.zeros_like(overlay)\n",
    "    pred_colored[:, :, 2] = pred_mask  # Blue for pred\n",
    "    \n",
    "    overlay = overlay * (1 - alpha)\n",
    "    overlay = overlay + gt_colored * (alpha * gt_mask[:, :, np.newaxis])\n",
    "    overlay = overlay + pred_colored * (alpha * pred_mask[:, :, np.newaxis])\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    axes[3].imshow(overlay)\n",
    "    axes[3].set_title('Overlay\\n(Red=GT, Blue=Pred)', fontsize=12, fontweight='bold')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    fig.suptitle(f'{img_name}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight', format='jpg')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Process each test dataset\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in TEST_DATASETS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROCESSING DATASET: {dataset_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = UnifiedDataset(\n",
    "        dataset_root=config.DATASET_ROOT,\n",
    "        img_size=config.IMG_SIZE,\n",
    "        transform=None,\n",
    "        joint_transform=None,\n",
    "        mask_blur_radius=0.0,\n",
    "        split='test',\n",
    "        datasets_to_load=[dataset_name],\n",
    "        datasets_to_skip=[],\n",
    "        filter_by_mask_area=False\n",
    "    )\n",
    "    \n",
    "    if len(test_dataset) == 0:\n",
    "        print(f\"⚠️ No images found in {dataset_name}/test, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(test_dataset)} test images\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"Running inference...\")\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    image_paths = []\n",
    "    iou_per_image = []\n",
    "    f1_per_image = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks, class_labels, weight_maps) in enumerate(tqdm(test_loader, desc=f\"Inference {dataset_name}\")):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = inference_model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predictions = (probs > 0.5).float()\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_targets.append(masks.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            \n",
    "            # Get image paths from dataset based on batch indices\n",
    "            batch_size = images.shape[0]\n",
    "            start_idx = batch_idx * test_loader.batch_size\n",
    "            for i in range(batch_size):\n",
    "                if start_idx + i < len(test_dataset.image_paths):\n",
    "                    image_paths.append(test_dataset.image_paths[start_idx + i])\n",
    "            \n",
    "            # Calculate per-image metrics\n",
    "            for i in range(batch_size):\n",
    "                pred_i = predictions[i:i+1]\n",
    "                target_i = masks[i:i+1]\n",
    "                \n",
    "                # IoU\n",
    "                intersection = (pred_i * target_i).sum()\n",
    "                union = pred_i.sum() + target_i.sum() - intersection\n",
    "                iou = (intersection / (union + 1e-6)).item() if union > 0 else 0.0\n",
    "                \n",
    "                # F1\n",
    "                pred_flat = pred_i.view(-1)\n",
    "                target_flat = target_i.view(-1)\n",
    "                tp = (pred_flat * target_flat).sum().item()\n",
    "                fp = (pred_flat * (1 - target_flat)).sum().item()\n",
    "                fn = ((1 - pred_flat) * target_flat).sum().item()\n",
    "                precision = (tp + 1e-6) / (tp + fp + 1e-6)\n",
    "                recall = (tp + 1e-6) / (tp + fn + 1e-6)\n",
    "                f1 = (2 * precision * recall) / (precision + recall + 1e-6)\n",
    "                \n",
    "                iou_per_image.append(iou)\n",
    "                f1_per_image.append(f1)\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    pred_flat = torch.cat(all_predictions, dim=0).view(-1).numpy()\n",
    "    target_flat = torch.cat(all_targets, dim=0).view(-1).numpy()\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    # Pixel-level metrics\n",
    "    tp = ((pred_flat == 1) & (target_flat == 1)).sum()\n",
    "    fp = ((pred_flat == 1) & (target_flat == 0)).sum()\n",
    "    fn = ((pred_flat == 0) & (target_flat == 1)).sum()\n",
    "    tn = ((pred_flat == 0) & (target_flat == 0)).sum()\n",
    "    \n",
    "    pixel_precision = tp / (tp + fp + 1e-6)\n",
    "    pixel_recall = tp / (tp + fn + 1e-6)\n",
    "    pixel_f1 = 2 * (pixel_precision * pixel_recall) / (pixel_precision + pixel_recall + 1e-6)\n",
    "    pixel_accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "    \n",
    "    # IoU (forgery class)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    union = pred_flat.sum() + target_flat.sum() - intersection\n",
    "    forgery_iou = (intersection / (union + 1e-6)) if union > 0 else 0.0\n",
    "    \n",
    "    # Image-level metrics\n",
    "    macro_image_f1 = np.mean(f1_per_image) if len(f1_per_image) > 0 else 0.0\n",
    "    mean_iou = np.mean(iou_per_image) if len(iou_per_image) > 0 else 0.0\n",
    "    \n",
    "    # Calculate image-level predictions (image has forgery if any forgery pixels)\n",
    "    image_pred_forgery = []\n",
    "    image_gt_forgery = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        batch_idx = i // test_loader.batch_size\n",
    "        pos_in_batch = i % test_loader.batch_size\n",
    "        if batch_idx < len(all_predictions) and pos_in_batch < all_predictions[batch_idx].shape[0]:\n",
    "            pred_img = all_predictions[batch_idx][pos_in_batch]\n",
    "            target_img = all_targets[batch_idx][pos_in_batch]\n",
    "            image_pred_forgery.append(pred_img.sum().item() > 0)\n",
    "            image_gt_forgery.append(target_img.sum().item() > 0)\n",
    "    \n",
    "    # Calculate image-level confusion matrix\n",
    "    image_pred_forgery = np.array(image_pred_forgery)\n",
    "    image_gt_forgery = np.array(image_gt_forgery)\n",
    "    cm_image = confusion_matrix(image_gt_forgery, image_pred_forgery, labels=[False, True])\n",
    "    tn_i, fp_i, fn_i, tp_i = cm_image[0,0], cm_image[0,1], cm_image[1,0], cm_image[1,1]\n",
    "    image_accuracy = (tp_i + tn_i) / (tp_i + tn_i + fp_i + fn_i + 1e-6)\n",
    "    image_precision = tp_i / (tp_i + fp_i + 1e-6)\n",
    "    image_recall = tp_i / (tp_i + fn_i + 1e-6)\n",
    "    image_f1 = 2 * (image_precision * image_recall) / (image_precision + image_recall + 1e-6)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[dataset_name] = {\n",
    "        'pixel_f1': pixel_f1,\n",
    "        'pixel_precision': pixel_precision,\n",
    "        'pixel_recall': pixel_recall,\n",
    "        'pixel_accuracy': pixel_accuracy,\n",
    "        'forgery_iou': forgery_iou,\n",
    "        'macro_image_f1': macro_image_f1,\n",
    "        'mean_iou': mean_iou,\n",
    "        'image_accuracy': image_accuracy,\n",
    "        'image_precision': image_precision,\n",
    "        'image_recall': image_recall,\n",
    "        'image_f1': image_f1,\n",
    "        'tp': int(tp),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'tn': int(tn),\n",
    "        'tp_i': int(tp_i),\n",
    "        'fp_i': int(fp_i),\n",
    "        'fn_i': int(fn_i),\n",
    "        'tn_i': int(tn_i),\n",
    "        'cm_image': cm_image,\n",
    "        'total_images': len(test_dataset),\n",
    "        'iou_per_image': iou_per_image,\n",
    "        'f1_per_image': f1_per_image,\n",
    "        'image_paths': image_paths,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "    \n",
    "    # Print comprehensive review\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"COMPREHENSIVE REVIEW: {dataset_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nDataset: {dataset_name}/test\")\n",
    "    print(f\"Total Images: {len(test_dataset)}\")\n",
    "    print(f\"\\n📊 OVERALL METRICS:\")\n",
    "    print(f\"  Forgery IoU:        {forgery_iou:.4f}\")\n",
    "    print(f\"  Pixel-wise F1:      {pixel_f1:.4f}\")\n",
    "    print(f\"  Macro Image F1:     {macro_image_f1:.4f}\")\n",
    "    print(f\"  Mean IoU per Image: {mean_iou:.4f}\")\n",
    "    print(f\"\\n📈 PIXEL-LEVEL STATISTICS:\")\n",
    "    print(f\"  Accuracy:           {pixel_accuracy:.4f}\")\n",
    "    print(f\"  Precision:          {pixel_precision:.4f}\")\n",
    "    print(f\"  Recall:             {pixel_recall:.4f}\")\n",
    "    print(f\"\\n🔢 CONFUSION MATRIX (Pixel-level):\")\n",
    "    print(f\"  {'':>15} {'Predicted 0':>15} {'Predicted 1':>15}\")\n",
    "    print(f\"  {'Actual 0':>15} {tn:>15,} {fp:>15,}\")\n",
    "    print(f\"  {'Actual 1':>15} {fn:>15,} {tp:>15,}\")\n",
    "    print(f\"\\n  Total Pixels:       {len(pred_flat):,}\")\n",
    "    print(f\"  Forgery Pixels:     {int(target_flat.sum()):,} ({100*target_flat.sum()/len(target_flat):.2f}%)\")\n",
    "    print(f\"\\n📊 IMAGE-LEVEL STATISTICS:\")\n",
    "    print(f\"  Accuracy:           {image_accuracy:.4f}\")\n",
    "    print(f\"  Precision:          {image_precision:.4f}\")\n",
    "    print(f\"  Recall:             {image_recall:.4f}\")\n",
    "    print(f\"  F1 Score:           {image_f1:.4f}\")\n",
    "    print(f\"\\n🔢 CONFUSION MATRIX (Image-level):\")\n",
    "    print(f\"  {'':>15} {'Predicted 0':>15} {'Predicted 1':>15}\")\n",
    "    print(f\"  {'Actual 0':>15} {tn_i:>15,} {fp_i:>15,}\")\n",
    "    print(f\"  {'Actual 1':>15} {fn_i:>15,} {tp_i:>15,}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create output directory\n",
    "    cm_output_dir = VIZ_OUTPUT_DIR / dataset_name\n",
    "    cm_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save Image-level confusion matrix\n",
    "    fig_image, ax_image = plt.subplots(figsize=(8, 6))\n",
    "    disp_image = ConfusionMatrixDisplay(confusion_matrix=cm_image, \n",
    "                                       display_labels=['Authentic', 'Forgery'])\n",
    "    disp_image.plot(ax=ax_image, cmap='Blues', values_format='d')\n",
    "    ax_image.set_title(f'{dataset_name.upper()} - Image-Level Confusion Matrix\\nAccuracy: {image_accuracy:.4f}, F1: {image_f1:.4f}', \n",
    "                      fontsize=14, fontweight='bold', pad=20)\n",
    "    ax_image.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "    ax_image.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add metrics text box\n",
    "    metrics_text_image = f\"\"\"Image-Level Metrics:\n",
    "    \n",
    "Accuracy:  {image_accuracy:.4f}\n",
    "Precision: {image_precision:.4f}\n",
    "Recall:    {image_recall:.4f}\n",
    "F1 Score:  {image_f1:.4f}\n",
    "\n",
    "Total Images: {len(test_dataset)}\"\"\"\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax_image.text(1.02, 0.5, metrics_text_image, transform=ax_image.transAxes, fontsize=10,\n",
    "                 verticalalignment='center', bbox=props, family='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_image_path = cm_output_dir / f'{dataset_name}_image_level_confusion_matrix.png'\n",
    "    plt.savefig(cm_image_path, dpi=150, bbox_inches='tight', format='png')\n",
    "    plt.close(fig_image)\n",
    "    print(f\"\\n💾 Image-level confusion matrix saved to: {cm_image_path}\")\n",
    "    \n",
    "    # Save Pixel-level confusion matrix\n",
    "    cm_pixel = np.array([[tn, fp], [fn, tp]])\n",
    "    fig_pixel, ax_pixel = plt.subplots(figsize=(8, 6))\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm_pixel, display_labels=['Authentic', 'Forgery'])\n",
    "    cm_display.plot(ax=ax_pixel, cmap='Blues', values_format='d')\n",
    "    ax_pixel.set_title(f'{dataset_name.upper()} - Pixel-Level Confusion Matrix\\nIoU: {forgery_iou:.4f}, F1: {pixel_f1:.4f}', \n",
    "                      fontsize=14, fontweight='bold', pad=20)\n",
    "    ax_pixel.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "    ax_pixel.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add metrics text box\n",
    "    metrics_text_pixel = f\"\"\"Pixel-Level Metrics:\n",
    "    \n",
    "Forgery IoU:     {forgery_iou:.4f}\n",
    "Pixel F1:        {pixel_f1:.4f}\n",
    "Macro Image F1:  {macro_image_f1:.4f}\n",
    "Mean IoU:        {mean_iou:.4f}\n",
    "\n",
    "Pixel-level:\n",
    "  Accuracy:      {pixel_accuracy:.4f}\n",
    "  Precision:     {pixel_precision:.4f}\n",
    "  Recall:        {pixel_recall:.4f}\n",
    "  \n",
    "Total Pixels:    {len(pred_flat):,}\n",
    "Forgery Pixels:  {int(target_flat.sum()):,} ({100*target_flat.sum()/len(target_flat):.2f}%)\"\"\"\n",
    "    \n",
    "    ax_pixel.text(1.02, 0.5, metrics_text_pixel, transform=ax_pixel.transAxes, fontsize=10,\n",
    "                 verticalalignment='center', bbox=props, family='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_pixel_path = cm_output_dir / f'{dataset_name}_pixel_level_confusion_matrix.png'\n",
    "    plt.savefig(cm_pixel_path, dpi=150, bbox_inches='tight', format='png')\n",
    "    plt.close(fig_pixel)\n",
    "    print(f\"💾 Pixel-level confusion matrix saved to: {cm_pixel_path}\")\n",
    "    \n",
    "    # Save first 10 image strips\n",
    "    print(f\"\\n💾 Saving first 10 image strips for {dataset_name}...\")\n",
    "    dataset_viz_dir = VIZ_OUTPUT_DIR / dataset_name\n",
    "    dataset_viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    num_strips = min(10, len(test_dataset))\n",
    "    saved_count = 0\n",
    "    \n",
    "    for img_idx in range(num_strips):\n",
    "        try:\n",
    "            # Get image data\n",
    "            img_path = image_paths[img_idx]\n",
    "            \n",
    "            # Find which batch and position this image is in\n",
    "            batch_idx = img_idx // 8\n",
    "            pos_in_batch = img_idx % 8\n",
    "            \n",
    "            if batch_idx >= len(all_predictions):\n",
    "                continue\n",
    "                \n",
    "            pred_batch = all_predictions[batch_idx]  # Shape: (batch_size, 1, H, W)\n",
    "            target_batch = all_targets[batch_idx]    # Shape: (batch_size, 1, H, W)\n",
    "            \n",
    "            if pos_in_batch >= pred_batch.shape[0]:\n",
    "                continue\n",
    "                \n",
    "            pred_tensor = pred_batch[pos_in_batch]   # Shape: (1, H, W)\n",
    "            target_tensor = target_batch[pos_in_batch]  # Shape: (1, H, W)\n",
    "            \n",
    "            # Load original image\n",
    "            img_bgr = cv2.imread(str(img_path))\n",
    "            if img_bgr is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Convert masks to numpy\n",
    "            pred_mask = pred_tensor.squeeze().numpy()\n",
    "            gt_mask = target_tensor.squeeze().numpy()\n",
    "            \n",
    "            # Resize masks to original image size if needed\n",
    "            if pred_mask.shape != img_rgb.shape[:2]:\n",
    "                pred_mask = cv2.resize(pred_mask, (img_rgb.shape[1], img_rgb.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                gt_mask = cv2.resize(gt_mask, (img_rgb.shape[1], img_rgb.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Create and save strip\n",
    "            img_name = Path(img_path).stem\n",
    "            output_path = dataset_viz_dir / f\"{img_idx+1:02d}_{img_name}.jpg\"\n",
    "            create_image_strip(img_rgb, gt_mask, pred_mask, output_path, img_name)\n",
    "            saved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error saving strip {img_idx+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"  ✓ Saved {saved_count} image strips to {dataset_viz_dir}\")\n",
    "\n",
    "# Print overall summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL SUMMARY - ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for dataset_name in TEST_DATASETS:\n",
    "    if dataset_name in all_results:\n",
    "        r = all_results[dataset_name]\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Images': r['total_images'],\n",
    "            'Mean IoU': f\"{r['mean_iou']:.4f}\",\n",
    "            'Pixel F1': f\"{r['pixel_f1']:.4f}\",\n",
    "            'Image F1': f\"{r['image_f1']:.4f}\",\n",
    "            'Pixel Prec': f\"{r['pixel_precision']:.4f}\",\n",
    "            'Pixel Rec': f\"{r['pixel_recall']:.4f}\"\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate averages\n",
    "    if len(summary_data) > 1:\n",
    "        avg_mean_iou = np.mean([all_results[d]['mean_iou'] for d in TEST_DATASETS if d in all_results])\n",
    "        avg_f1 = np.mean([all_results[d]['pixel_f1'] for d in TEST_DATASETS if d in all_results])\n",
    "        avg_image_f1 = np.mean([all_results[d]['image_f1'] for d in TEST_DATASETS if d in all_results])\n",
    "        \n",
    "        print(f\"\\n📊 AVERAGE ACROSS ALL DATASETS:\")\n",
    "        print(f\"  Average Mean IoU:     {avg_mean_iou:.4f}\")\n",
    "        print(f\"  Average Pixel F1:     {avg_f1:.4f}\")\n",
    "        print(f\"  Average Image F1:     {avg_image_f1:.4f}\")\n",
    "\n",
    "# Create combined confusion matrix visualization for all datasets\n",
    "if len(all_results) > 0:\n",
    "    print(\"\\n💾 Creating combined confusion matrix visualization...\")\n",
    "    # Create 2 rows: image-level (top) and pixel-level (bottom)\n",
    "    fig, axes = plt.subplots(2, len(all_results), figsize=(6 * len(all_results), 12))\n",
    "    if len(all_results) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for idx, dataset_name in enumerate(TEST_DATASETS):\n",
    "        if dataset_name not in all_results:\n",
    "            continue\n",
    "        \n",
    "        r = all_results[dataset_name]\n",
    "        \n",
    "        # Image-level confusion matrix (top row)\n",
    "        cm_image = r.get('cm_image', np.array([[r['tn_i'], r['fp_i']], [r['fn_i'], r['tp_i']]]))\n",
    "        disp_image = ConfusionMatrixDisplay(confusion_matrix=cm_image, \n",
    "                                           display_labels=['Authentic', 'Forgery'])\n",
    "        disp_image.plot(ax=axes[0, idx], cmap='Blues', values_format='d')\n",
    "        axes[0, idx].set_title(f'{dataset_name}\\nImage-Level (F1: {r.get(\"image_f1\", 0):.3f})', \n",
    "                              fontsize=11, fontweight='bold')\n",
    "        axes[0, idx].set_xlabel('Predicted', fontsize=10)\n",
    "        axes[0, idx].set_ylabel('Actual', fontsize=10)\n",
    "        \n",
    "        # Pixel-level confusion matrix (bottom row)\n",
    "        cm_pixel = np.array([[r['tn'], r['fp']], [r['fn'], r['tp']]])\n",
    "        cm_display = ConfusionMatrixDisplay(confusion_matrix=cm_pixel, display_labels=['Authentic', 'Forgery'])\n",
    "        cm_display.plot(ax=axes[1, idx], cmap='Blues', values_format='d')\n",
    "        axes[1, idx].set_title(f'{dataset_name}\\nPixel-Level (F1: {r[\"pixel_f1\"]:.3f})', \n",
    "                              fontsize=11, fontweight='bold')\n",
    "        axes[1, idx].set_xlabel('Predicted', fontsize=10)\n",
    "        axes[1, idx].set_ylabel('Actual', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Combined Confusion Matrices - All Test Datasets', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save combined confusion matrix\n",
    "    combined_cm_path = VIZ_OUTPUT_DIR / 'combined_confusion_matrices.png'\n",
    "    plt.savefig(combined_cm_path, dpi=150, bbox_inches='tight', format='png')\n",
    "    plt.close(fig)\n",
    "    print(f\"  ✓ Combined confusion matrix saved to: {combined_cm_path}\")\n",
    "\n",
    "print(f\"\\n💾 Visualizations saved to: {VIZ_OUTPUT_DIR}\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN/3bJq0t7seGZVjQAMTzsd",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
